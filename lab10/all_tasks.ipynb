{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de223d0-ef96-4c69-8f91-efd7434de8e8",
   "metadata": {},
   "source": [
    "#### Задание на ПР-10:\n",
    "1. Скачайте датасет с рукописными цифрами. Подойдет MNIST или его часть с не менее чем 10000 изображениями. Проследите чтобы число изображений на 1 цифру было примерно равным.\n",
    "2. Напишите кодировщик, который будет переводить изображение (того формата, который вы скачали) в числовой вектор.\n",
    "3. Сформируйте числовой датасет, в последнем столбце которого поставьте метку - цифру, которая изображена.\n",
    "4. Напишите однослойную нейронную сеть с сигмоидой в качестве функции активации. Число нейронов выберите равным 10, число их входов выберите так, чтобы ваш вектор данных входил в сеть. \n",
    "5. Обучите сеть так чтобы возбуждение n-го нейрона соответствовало цифре n (нейроны нумеруем от 0 до 9). \n",
    "6. Повторите шаги 4 и 5 для двуслойной нейронной сети с промежуточным слоем из 25 нейронов и разными функциями активации (на ваш выбор).\n",
    "7. Сформируйте предсказания на 1-слойной и 2-слойной ИНС и посчитайте accuracy в обоих случаях. \n",
    "8. Сделайте выводы о работе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eabe4c7-799d-4a31-9a9e-7c90e633610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные загружены успешно\n"
     ]
    }
   ],
   "source": [
    "# Импорт библиотек и загрузка данных\n",
    "\n",
    "# Импортируем библиотеку numpy для работы с массивами и матрицами\n",
    "import numpy as np\n",
    "\n",
    "# Импортируем библиотеку pandas для работы с табличными данными\n",
    "import pandas as pd\n",
    "\n",
    "# Импортируем модуль random для генерации случайных чисел\n",
    "import random\n",
    "\n",
    "# Импортируем модуль math для математических функций\n",
    "import math\n",
    "\n",
    "# Функция для загрузки данных из CSV файла\n",
    "def load_data_from_file(filename):\n",
    "    # Используем pandas для чтения CSV файла\n",
    "    # Параметр header=None указывает, что в файле нет строки заголовков\n",
    "    data_frame = pd.read_csv(filename, header=None)\n",
    "    # Возвращаем загруженные данные в виде DataFrame\n",
    "    return data_frame\n",
    "\n",
    "# Загружаем данные MNIST из файла 'mnist_test.csv'\n",
    "mnist_dataset = load_data_from_file('mnist_test.csv')\n",
    "\n",
    "# Выводим сообщение об успешной загрузке данных\n",
    "print(\"Данные загружены успешно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8a0d6a-f743-4466-830f-bd0324f9d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для обработки изображений и создания датасета\n",
    "\n",
    "# Функция для нормализации значений пикселей\n",
    "def normalize_pixels(pixel_data):\n",
    "    # Создаем пустой список для хранения нормализованных значений\n",
    "    normalized_pixels = []\n",
    "    \n",
    "    # Проходим по каждому пикселю в массиве pixel_data(784)\n",
    "    for pixel in pixel_data:\n",
    "        # Нормализуем значение пикселя: делим на 255\n",
    "        # Это преобразует значения из диапазона 0-255 в диапазон 0-1\n",
    "        normalized_pixel = pixel / 255.0\n",
    "        # Добавляем нормализованное значение в список\n",
    "        normalized_pixels.append(normalized_pixel)\n",
    "    \n",
    "    # Возвращаем список нормализованных пикселей\n",
    "    return normalized_pixels\n",
    "\n",
    "# Функция для создания обучающего датасета из загруженных данных\n",
    "def create_training_dataset(data):\n",
    "    # Создаем пустой список для векторов признаков (изображений)\n",
    "    feature_vectors = []\n",
    "    \n",
    "    # Создаем пустой список для меток классов (цифр)\n",
    "    target_labels = []\n",
    "    \n",
    "    # Получаем количество строк в данных\n",
    "    data_length = len(data)\n",
    "    \n",
    "    # Проходим по всем строкам данных\n",
    "    for row_index in range(data_length):\n",
    "        # Получаем значения текущей строки\n",
    "        row_values = data.iloc[row_index].values\n",
    "        \n",
    "        # Первое значение в строке - метка цифры (0-9)\n",
    "        # Преобразуем его в целое число\n",
    "        digit_label = int(row_values[0])\n",
    "        \n",
    "        # Остальные значения в строке - пиксели изображения\n",
    "        # Используем срез [1:] чтобы получить все значения кроме первого\n",
    "        image_pixels = row_values[1:]\n",
    "        \n",
    "        # Нормализуем пиксели изображения\n",
    "        normalized_vector = normalize_pixels(image_pixels)\n",
    "        \n",
    "        # Добавляем нормализованный вектор в список признаков\n",
    "        feature_vectors.append(normalized_vector)\n",
    "        \n",
    "        # Добавляем метку класса в список меток\n",
    "        target_labels.append(digit_label)\n",
    "    \n",
    "    # Преобразуем списки в массивы numpy и возвращаем их\n",
    "    return np.array(feature_vectors), np.array(target_labels)\n",
    "\n",
    "# Создаем датасет из загруженных данных MNIST\n",
    "X_features, y_labels = create_training_dataset(mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe79ccc1-a95f-49d0-9d75-4deab24ed744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анализ распределения цифр в датасете:\n",
      "Цифра 0: 980 изображений\n",
      "Цифра 1: 1135 изображений\n",
      "Цифра 2: 1032 изображений\n",
      "Цифра 3: 1010 изображений\n",
      "Цифра 4: 982 изображений\n",
      "Цифра 5: 892 изображений\n",
      "Цифра 6: 958 изображений\n",
      "Цифра 7: 1028 изображений\n",
      "Цифра 8: 974 изображений\n",
      "Цифра 9: 1009 изображений\n",
      "Обучающая выборка: 8000 образцов\n",
      "Тестовая выборка: 2000 образцов\n"
     ]
    }
   ],
   "source": [
    "# Анализ данных и разделение на выборки\n",
    "\n",
    "# Выводим заголовок для анализа распределения цифр\n",
    "print(\"Анализ распределения цифр в датасете:\")\n",
    "\n",
    "# Создаем пустой словарь для подсчета количества каждой цифры\n",
    "digit_distribution = {}\n",
    "\n",
    "# Проходим по всем меткам в датасете\n",
    "for digit in y_labels:\n",
    "    # Проверяем, есть ли уже такая цифра в словаре\n",
    "    if digit in digit_distribution:\n",
    "        # Если есть, увеличиваем счетчик на 1\n",
    "        digit_distribution[digit] = digit_distribution[digit] + 1\n",
    "    else:\n",
    "        # Если нет, добавляем новую запись со значением 1\n",
    "        digit_distribution[digit] = 1\n",
    "\n",
    "# Получаем отсортированный список ключей словаря\n",
    "sorted_digits = sorted(digit_distribution.keys())\n",
    "\n",
    "# Проходим по отсортированным цифрам\n",
    "for digit in sorted_digits:\n",
    "    # Получаем количество изображений для текущей цифры\n",
    "    count = digit_distribution[digit]\n",
    "    # Выводим информацию о количестве изображений для каждой цифры\n",
    "    print(f\"Цифра {digit}: {count} изображений\")\n",
    "\n",
    "# Функция для разделения данных на обучающую и тестовую выборки\n",
    "def split_dataset(features, labels, test_fraction=0.2):\n",
    "    # Получаем общее количество образцов\n",
    "    total_samples = len(features)\n",
    "    \n",
    "    # Вычисляем количество тестовых образцов\n",
    "    # test_fraction=0.2 означает 20% тестовых данных\n",
    "    test_samples_count = int(total_samples * test_fraction)\n",
    "    \n",
    "    # Создаем список индексов от 0 до total_samples-1\n",
    "    indices_list = []\n",
    "    for i in range(total_samples):\n",
    "        indices_list.append(i)\n",
    "    \n",
    "    # Перемешиваем список индексов случайным образом\n",
    "    # Это важно для случайного распределения данных\n",
    "    random.shuffle(indices_list)\n",
    "    \n",
    "    # Выбираем индексы для тестовой выборки (первые test_samples_count)\n",
    "    test_indices = indices_list[:test_samples_count]\n",
    "    \n",
    "    # Выбираем индексы для обучающей выборки (остальные)\n",
    "    train_indices = indices_list[test_samples_count:]\n",
    "    \n",
    "    # Создаем обучающую выборку признаков по выбранным индексам\n",
    "    X_train = features[train_indices]\n",
    "    \n",
    "    # Создаем тестовую выборку признаков по выбранным индексам\n",
    "    X_test = features[test_indices]\n",
    "    \n",
    "    # Создаем обучающую выборку меток по выбранным индексам\n",
    "    y_train = labels[train_indices]\n",
    "    \n",
    "    # Создаем тестовую выборку меток по выбранным индексам\n",
    "    y_test = labels[test_indices]\n",
    "    \n",
    "    # Возвращаем все четыре выборки\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = split_dataset(X_features, y_labels)\n",
    "\n",
    "# Выводим размеры обучающей и тестовой выборок\n",
    "print(f\"Обучающая выборка: {len(X_train)} образцов\")\n",
    "print(f\"Тестовая выборка: {len(X_test)} образцов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04287804-3dd7-4208-bde1-97f7cdce522a",
   "metadata": {},
   "source": [
    "**Формула обновления весов:** новые_веса = старые_веса - скорость_обучения × градиент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9d03a3-f3c2-4127-9743-dd7bce0ba508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции активации\n",
    "\n",
    "# Сигмоидная функция активации для скалярных значений\n",
    "def sigmoid_activation(x):\n",
    "    # Проверяем, не слишком ли большое значение x\n",
    "    if x > 100:\n",
    "        # Если x больше 100, возвращаем 1.0 (сигмоид стремится к 1)\n",
    "        return 1.0\n",
    "    # Проверяем, не слишком ли маленькое значение x\n",
    "    elif x < -100:\n",
    "        # Если x меньше -100, возвращаем 0.0 (сигмоид стремится к 0)\n",
    "        return 0.0\n",
    "    else:\n",
    "        # Для нормальных значений вычисляем сигмоид по формуле\n",
    "        return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "# Производная сигмоидной функции для скалярных значений\n",
    "def sigmoid_derivative(x):\n",
    "    # Производная сигмоида вычисляется как f'(x) = f(x) * (1 - f(x))\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "# ReLU функция активации для скалярных значений\n",
    "def relu_activation(x):\n",
    "    # Проверяем, положительное ли значение x\n",
    "    if x > 0:\n",
    "        # Если x положительное, возвращаем x без изменений\n",
    "        return x\n",
    "    else:\n",
    "        # Если x отрицательное или ноль, возвращаем 0.0\n",
    "        return 0.0\n",
    "\n",
    "# Производная ReLU функции для скалярных значений\n",
    "def relu_derivative(x):\n",
    "    # Проверяем, положительное ли значение x\n",
    "    if x > 0:\n",
    "        # Если x положительное, производная равна 1.0\n",
    "        return 1.0\n",
    "    else:\n",
    "        # Если x отрицательное или ноль, производная равна 0.0\n",
    "        return 0.0\n",
    "\n",
    "# Векторизованная версия сигмоидной функции для массивов\n",
    "def vectorized_sigmoid(x_array):\n",
    "    # Проверяем размерность входного массива\n",
    "    if x_array.ndim > 1:\n",
    "        # Если массив двумерный, создаем пустой список для результата\n",
    "        result = []\n",
    "        # Получаем количество строк в массиве\n",
    "        num_rows = x_array.shape[0]\n",
    "        # Проходим по каждой строке массива\n",
    "        for row_index in range(num_rows):\n",
    "            # Получаем текущую строку\n",
    "            current_row = x_array[row_index]\n",
    "            # Создаем пустой список для результатов текущей строки\n",
    "            row_result = []\n",
    "            # Получаем количество элементов в строке\n",
    "            row_length = len(current_row)\n",
    "            # Проходим по каждому элементу строки\n",
    "            for element_index in range(row_length):\n",
    "                # Применяем сигмоид к текущему элементу\n",
    "                element_result = sigmoid_activation(current_row[element_index])\n",
    "                # Добавляем результат в список строки\n",
    "                row_result.append(element_result)\n",
    "            # Добавляем список строки в общий результат\n",
    "            result.append(row_result)\n",
    "        # Преобразуем список в массив numpy\n",
    "        return np.array(result)\n",
    "    else:\n",
    "        # Если массив одномерный, создаем пустой список для результата\n",
    "        result = []\n",
    "        # Получаем длину массива\n",
    "        array_length = len(x_array)\n",
    "        # Проходим по каждому элементу массива\n",
    "        for element_index in range(array_length):\n",
    "            # Применяем сигмоид к текущему элементу\n",
    "            element_result = sigmoid_activation(x_array[element_index])\n",
    "            # Добавляем результат в список\n",
    "            result.append(element_result)\n",
    "        # Преобразуем список в массив numpy\n",
    "        return np.array(result)\n",
    "\n",
    "# Векторизованная версия производной сигмоидной функции\n",
    "def vectorized_sigmoid_derivative(x_array):\n",
    "    # Проверяем размерность входного массива\n",
    "    if x_array.ndim > 1:\n",
    "        # Если массив двумерный, создаем пустой список для результата\n",
    "        result = []\n",
    "        # Получаем количество строк в массиве\n",
    "        num_rows = x_array.shape[0]\n",
    "        # Проходим по каждой строке массива\n",
    "        for row_index in range(num_rows):\n",
    "            # Получаем текущую строку\n",
    "            current_row = x_array[row_index]\n",
    "            # Создаем пустой список для результатов текущей строки\n",
    "            row_result = []\n",
    "            # Получаем количество элементов в строке\n",
    "            row_length = len(current_row)\n",
    "            # Проходим по каждому элементу строки\n",
    "            for element_index in range(row_length):\n",
    "                # Применяем производную сигмоида к текущему элементу\n",
    "                element_result = sigmoid_derivative(current_row[element_index])\n",
    "                # Добавляем результат в список строки\n",
    "                row_result.append(element_result)\n",
    "            # Добавляем список строки в общий результат\n",
    "            result.append(row_result)\n",
    "        # Преобразуем список в массив numpy\n",
    "        return np.array(result)\n",
    "    else:\n",
    "        # Если массив одномерный, создаем пустой список для результата\n",
    "        result = []\n",
    "        # Получаем длину массива\n",
    "        array_length = len(x_array)\n",
    "        # Проходим по каждому элементу массива\n",
    "        for element_index in range(array_length):\n",
    "            # Применяем производную сигмоида к текущему элементу\n",
    "            element_result = sigmoid_derivative(x_array[element_index])\n",
    "            # Добавляем результат в список\n",
    "            result.append(element_result)\n",
    "        # Преобразуем список в массив numpy\n",
    "        return np.array(result)\n",
    "\n",
    "# Векторизованная версия ReLU функции\n",
    "def vectorized_relu(x_array):\n",
    "    # Проверяем размерность входного массива\n",
    "    if x_array.ndim > 1:\n",
    "        # Если массив двумерный, создаем пустой список для результата\n",
    "        result = []\n",
    "        # Получаем количество строк в массиве\n",
    "        num_rows = x_array.shape[0]\n",
    "        # Проходим по каждой строке массива\n",
    "        for row_index in range(num_rows):\n",
    "            # Получаем текущую строку\n",
    "            current_row = x_array[row_index]\n",
    "            # Создаем пустой список для результатов текущей строки\n",
    "            row_result = []\n",
    "            # Получаем количество элементов в строке\n",
    "            row_length = len(current_row)\n",
    "            # Проходим по каждому элементу строки\n",
    "            for element_index in range(row_length):\n",
    "                # Применяем ReLU к текущему элементу\n",
    "                element_result = relu_activation(current_row[element_index])\n",
    "                # Добавляем результат в список строки\n",
    "                row_result.append(element_result)\n",
    "            # Добавляем список строки в общий результат\n",
    "            result.append(row_result)\n",
    "        # Преобразуем список в массив numpy\n",
    "        return np.array(result)\n",
    "    else:\n",
    "        # Если массив одномерный, создаем пустой список для результата\n",
    "        result = []\n",
    "        # Получаем длину массива\n",
    "        array_length = len(x_array)\n",
    "        # Проходим по каждому элементу массива\n",
    "        for element_index in range(array_length):\n",
    "            # Применяем ReLU к текущему элементу\n",
    "            element_result = relu_activation(x_array[element_index])\n",
    "            # Добавляем результат в список\n",
    "            result.append(element_result)\n",
    "        # Преобразуем список в массив numpy\n",
    "        return np.array(result)\n",
    "\n",
    "# Векторизованная версия производной ReLU функции\n",
    "def vectorized_relu_derivative(x_array):\n",
    "    # Проверяем размерность входного массива\n",
    "    if x_array.ndim > 1:\n",
    "        # Если массив двумерный, создаем пустой список для результата\n",
    "        result = []\n",
    "        # Получаем количество строк в массиве\n",
    "        num_rows = x_array.shape[0]\n",
    "        # Проходим по каждой строке массива\n",
    "        for row_index in range(num_rows):\n",
    "            # Получаем текущую строку\n",
    "            current_row = x_array[row_index]\n",
    "            # Создаем пустой список для результатов текущей строки\n",
    "            row_result = []\n",
    "            # Получаем количество элементов в строке\n",
    "            row_length = len(current_row)\n",
    "            # Проходим по каждому элементу строки\n",
    "            for element_index in range(row_length):\n",
    "                # Применяем производную ReLU к текущему элементу\n",
    "                element_result = relu_derivative(current_row[element_index])\n",
    "                # Добавляем результат в список строки\n",
    "                row_result.append(element_result)\n",
    "            # Добавляем список строки в общий результат\n",
    "            result.append(row_result)\n",
    "        # Преобразуем список в массив numpy\n",
    "        return np.array(result)\n",
    "    else:\n",
    "        # Если массив одномерный, создаем пустой список для результата\n",
    "        result = []\n",
    "        # Получаем длину массива\n",
    "        array_length = len(x_array)\n",
    "        # Проходим по каждому элементу массива\n",
    "        for element_index in range(array_length):\n",
    "            # Применяем производную ReLU к текущему элементу\n",
    "            element_result = relu_derivative(x_array[element_index])\n",
    "            # Добавляем результат в список\n",
    "            result.append(element_result)\n",
    "        # Преобразуем список в массив numpy\n",
    "        return np.array(result)\n",
    "\n",
    "# Softmax функция для многоклассовой классификации\n",
    "def softmax_activation(vector):\n",
    "    # Находим максимальное значение в векторе для численной стабильности\n",
    "    max_value = np.max(vector)\n",
    "    \n",
    "    # Создаем пустой список для экспоненцированных значений\n",
    "    exp_values = []\n",
    "    \n",
    "    # Получаем длину вектора\n",
    "    vector_length = len(vector)\n",
    "    \n",
    "    # Проходим по каждому элементу вектора\n",
    "    for element_index in range(vector_length):\n",
    "        # Вычисляем экспоненту от разности элемента и максимума\n",
    "        current_value = vector[element_index]\n",
    "        exp_value = math.exp(current_value - max_value)\n",
    "        # Добавляем результат в список\n",
    "        exp_values.append(exp_value)\n",
    "    \n",
    "    # Вычисляем сумму всех экспоненцированных значений\n",
    "    sum_exp = 0.0\n",
    "    for exp_val in exp_values:\n",
    "        sum_exp = sum_exp + exp_val\n",
    "    \n",
    "    # Создаем пустой список для результатов softmax\n",
    "    softmax_result = []\n",
    "    \n",
    "    # Проходим по всем экспоненцированным значениям\n",
    "    for exp_val in exp_values:\n",
    "        # Вычисляем softmax для каждого значения\n",
    "        softmax_value = exp_val / sum_exp\n",
    "        # Добавляем результат в список\n",
    "        softmax_result.append(softmax_value)\n",
    "    \n",
    "    # Преобразуем список в массив numpy\n",
    "    return np.array(softmax_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3069eda3-83f2-4aee-af6e-e6f32308c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для однослойной нейронной сети\n",
    "\n",
    "# Функция для инициализации параметров однослойной сети\n",
    "def initialize_single_layer_parameters(input_size, output_size):\n",
    "    # input_size = 784 (пикселей в изображении), output_size = 10 (цифр для распознавания\n",
    "    \n",
    "    # Инициализируем веса случайными маленькими значениями\n",
    "    # np.random.randn создает массив заданного размера со случайными значениями\n",
    "    # Умножаем на 0.01 чтобы начальные значения были маленькими\n",
    "    weights = np.random.randn(input_size, output_size) * 0.01\n",
    "    \n",
    "    # Инициализируем смещения нулями\n",
    "    biases = np.zeros(output_size)\n",
    "    \n",
    "    # Возвращаем инициализированные веса и смещения\n",
    "    return weights, biases\n",
    "\n",
    "# Функция прямого распространения для однослойной сети\n",
    "def single_layer_forward_propagation(input_data, weights, biases):\n",
    "    # Вычисляем взвешенную сумму: матричное умножение входов на веса\n",
    "    weighted_sum = np.dot(input_data, weights)\n",
    "    \n",
    "    # Добавляем смещения к взвешенной сумме\n",
    "    weighted_sum = weighted_sum + biases\n",
    "    \n",
    "    # Применяем сигмоидную функцию активации к результату\n",
    "    output = vectorized_sigmoid(weighted_sum)\n",
    "    \n",
    "    # Возвращаем выход сети\n",
    "    return output\n",
    "\n",
    "# Функция обратного распространения для однослойной сети\n",
    "def single_layer_backward_propagation(input_data, true_labels, network_output, weights, biases, learning_rate):\n",
    "    # Получаем размер батча (количество образцов)\n",
    "    batch_size = input_data.shape[0]\n",
    "    \n",
    "    # Получаем количество входных признаков\n",
    "    input_features = input_data.shape[1]\n",
    "    \n",
    "    # Получаем количество выходных нейронов\n",
    "    output_neurons = weights.shape[1]\n",
    "    \n",
    "    # Вычисляем ошибку между предсказанием сети и истинными метками\n",
    "    output_error = network_output - true_labels\n",
    "    \n",
    "    # Вычисляем градиент выходного слоя с учетом производной функции активации\n",
    "    # Умножаем ошибку на производную сигмоида от выхода сети\n",
    "    output_gradient = output_error * vectorized_sigmoid_derivative(network_output)\n",
    "    \n",
    "    # Вычисляем градиенты для весов \n",
    "    # Создаем массив для градиентов весов такого же размера как weights\n",
    "    weight_gradients = np.zeros((input_features, output_neurons))\n",
    "    \n",
    "    # Проходим по всем образцам в батче\n",
    "    for sample_index in range(batch_size):\n",
    "        # Получаем входные данные для текущего образца (784 значения)\n",
    "        current_input = input_data[sample_index]\n",
    "        \n",
    "        # Получаем градиент для текущего образца (10 значений)\n",
    "        current_gradient = output_gradient[sample_index]\n",
    "        \n",
    "        # Проходим по всем входным признакам (784 раза)\n",
    "        for feature_index in range(input_features):\n",
    "            # Проходим по всем выходным нейронам (10 раз)\n",
    "            for neuron_index in range(output_neurons):\n",
    "                # Градиент для веса = входной признак × градиент нейрона\n",
    "                # Это реализация формулы dW = X.T × dZ\n",
    "                weight_gradients[feature_index, neuron_index] += current_input[feature_index] * current_gradient[neuron_index]\n",
    "    \n",
    "    # Делим на размер батча для усреднения градиентов по всем образцам\n",
    "    weight_gradients = weight_gradients / batch_size\n",
    "    \n",
    "    # Вычисляем градиенты для смещений\n",
    "    bias_gradients = np.zeros(output_neurons)\n",
    "    \n",
    "    # Проходим по всем образцам в батче\n",
    "    for sample_index in range(batch_size):\n",
    "        # Получаем градиент для текущего образца\n",
    "        current_gradient = output_gradient[sample_index]\n",
    "        \n",
    "        # Проходим по всем выходным нейронам\n",
    "        for neuron_index in range(output_neurons):\n",
    "            # Градиент для смещения = градиент нейрона\n",
    "            # Это реализация формулы db = sum(dZ)\n",
    "            bias_gradients[neuron_index] += current_gradient[neuron_index]\n",
    "    \n",
    "    # Делим на размер батча для усреднения\n",
    "    bias_gradients = bias_gradients / batch_size\n",
    "    \n",
    "    # Обновляем веса с учетом скорости обучения\n",
    "    # Формула градиентного спуска: W = W - α × dW\n",
    "    weights = weights - (learning_rate * weight_gradients)\n",
    "    \n",
    "    # Обновляем смещения с учетом скорости обучения\n",
    "    biases = biases - (learning_rate * bias_gradients)\n",
    "    \n",
    "    # Возвращаем обновленные веса и смещения\n",
    "    return weights, biases\n",
    "\n",
    "# Функция для преобразования меток в one-hot encoding\n",
    "def convert_to_one_hot(labels, num_classes=10):\n",
    "    # Получаем количество образцов\n",
    "    num_samples = len(labels)\n",
    "    \n",
    "    # Создаем матрицу нулей размером num_samples x num_classes\n",
    "    one_hot_labels = np.zeros((num_samples, num_classes))\n",
    "    \n",
    "    # Проходим по всем образцам\n",
    "    for i in range(num_samples):\n",
    "        # Получаем метку текущего образца\n",
    "        current_label = labels[i]\n",
    "        # Устанавливаем 1.0 в столбце, соответствующем метке\n",
    "        one_hot_labels[i, current_label] = 1.0\n",
    "    \n",
    "    # Возвращаем one-hot представление меток\n",
    "    return one_hot_labels\n",
    "\n",
    "# Функция для обучения однослойной сети\n",
    "def train_single_layer_network(training_data, training_labels, input_size, output_size, learning_rate=0.1, epochs=500):\n",
    "    # Инициализируем параметры сети\n",
    "    weights, biases = initialize_single_layer_parameters(input_size, output_size)\n",
    "    \n",
    "    # Преобразуем метки в one-hot encoding\n",
    "    one_hot_labels = convert_to_one_hot(training_labels)\n",
    "    \n",
    "    # Получаем количество образцов для обучения\n",
    "    num_samples = len(training_data)\n",
    "    \n",
    "    # Цикл обучения по эпохам\n",
    "    for epoch in range(epochs):\n",
    "        # Прямой проход: получаем предсказания сети\n",
    "        predictions = single_layer_forward_propagation(training_data, weights, biases)\n",
    "        \n",
    "        # Обратное распространение: обновляем параметры сети\n",
    "        weights, biases = single_layer_backward_propagation(\n",
    "            training_data, one_hot_labels, predictions, weights, biases, learning_rate\n",
    "        )\n",
    "        \n",
    "        # Вывод прогресса обучения каждые 100 эпох\n",
    "        if epoch % 100 == 0:\n",
    "            # Инициализируем переменную для общей ошибки\n",
    "            total_error = 0.0\n",
    "            \n",
    "            # Вычисляем среднеквадратичную ошибку\n",
    "            for i in range(num_samples):\n",
    "                for j in range(output_size):\n",
    "                    # Вычисляем ошибку для каждого класса и образца\n",
    "                    error = predictions[i, j] - one_hot_labels[i, j]\n",
    "                    # Добавляем квадрат ошибки к общей ошибке\n",
    "                    total_error = total_error + (error * error)\n",
    "            \n",
    "            # Вычисляем среднюю ошибку\n",
    "            mean_error = total_error / (num_samples * output_size)\n",
    "            \n",
    "            # Выводим информацию о текущей эпохе и ошибке\n",
    "            print(f\"Эпоха {epoch}, Средняя ошибка: {mean_error:.4f}\")\n",
    "    \n",
    "    # Возвращаем обученные веса и смещения\n",
    "    return weights, biases\n",
    "\n",
    "# Функция для предсказания с использованием однослойной сети\n",
    "def predict_single_layer(input_data, weights, biases):\n",
    "    # Выполняем прямой проход для получения вероятностей\n",
    "    output_probabilities = single_layer_forward_propagation(input_data, weights, biases)\n",
    "    \n",
    "    # Создаем пустой список для предсказанных классов\n",
    "    predicted_classes = []\n",
    "    \n",
    "    # Получаем количество образцов\n",
    "    num_samples = output_probabilities.shape[0]\n",
    "    \n",
    "    # Проходим по всем образцам\n",
    "    for sample_index in range(num_samples):\n",
    "        # Получаем вероятности для текущего образца\n",
    "        current_probabilities = output_probabilities[sample_index]\n",
    "        \n",
    "        # Инициализируем лучший класс и максимальную вероятность\n",
    "        best_class = 0\n",
    "        highest_probability = current_probabilities[0]\n",
    "        \n",
    "        # Проходим по всем классам, начиная со второго\n",
    "        for class_index in range(1, len(current_probabilities)):\n",
    "            # Сравниваем текущую вероятность с максимальной\n",
    "            if current_probabilities[class_index] > highest_probability:\n",
    "                # Обновляем максимальную вероятность и лучший класс\n",
    "                highest_probability = current_probabilities[class_index]\n",
    "                best_class = class_index\n",
    "        \n",
    "        # Добавляем предсказанный класс в список\n",
    "        predicted_classes.append(best_class)\n",
    "    \n",
    "    # Преобразуем список в массив numpy\n",
    "    return np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc7e3f-04e0-4a33-a898-20c2115a89ab",
   "metadata": {},
   "source": [
    "**Алгоритм обучения:**\n",
    "\n",
    "Для каждой эпохи (500 раз):\n",
    "  1. Прямой проход:\n",
    "     - Вход × Веса + Смещение = Z\n",
    "     - Сигмоида(Z) = выход (10 вероятностей)\n",
    "   \n",
    "  2. Вычисление ошибки:\n",
    "     - Выход - One-hot метка = ошибка\n",
    "   \n",
    "  3. Обратное распространение: (найти градиенты)\n",
    "     - Градиент = ошибка × производная_сигмоиды\n",
    "     - dW = входᵀ × градиент / batch_size\n",
    "     - db = сумма(градиент) / batch_size\n",
    "   \n",
    "  4. Обновление весов: (градиентный спуск)\n",
    "     - Веса = Веса - 0.1 × dW\n",
    "     - Смещения = Смещения - 0.1 × db\n",
    "\n",
    "**Предсказание:**\n",
    "1. Прямой проход \n",
    "2. Найти нейрон с максимальным значением \n",
    "3. Его индекс = предсказанная цифра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73cb60dd-9042-4ce3-b3b4-891d017d2471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для двухслойной нейронной сети\n",
    "\n",
    "# Функция для инициализации параметров двухслойной сети\n",
    "def initialize_two_layer_parameters(input_size, hidden_size, output_size):\n",
    "    # Инициализируем веса первого слоя случайными маленькими значениями\n",
    "    weights1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "    \n",
    "    # Инициализируем смещения первого слоя нулями\n",
    "    biases1 = np.zeros(hidden_size)\n",
    "    \n",
    "    # Инициализируем веса второго слоя случайными маленькими значениями\n",
    "    weights2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "    \n",
    "    # Инициализируем смещения второго слоя нулями\n",
    "    biases2 = np.zeros(output_size)\n",
    "    \n",
    "    # Возвращаем все инициализированные параметры\n",
    "    return weights1, biases1, weights2, biases2\n",
    "\n",
    "# Функция прямого распространения для двухслойной сети\n",
    "def two_layer_forward_propagation(input_data, weights1, biases1, weights2, biases2):\n",
    "    # Первый слой: линейное преобразование\n",
    "    z1 = np.dot(input_data, weights1)\n",
    "    z1 = z1 + biases1\n",
    "    \n",
    "    # Применяем ReLU активацию к первому слою\n",
    "    a1 = vectorized_relu(z1)\n",
    "    \n",
    "    # Второй слой: линейное преобразование\n",
    "    z2 = np.dot(a1, weights2)\n",
    "    z2 = z2 + biases2\n",
    "    \n",
    "    # Создаем пустой список для выходных вероятностей\n",
    "    a2_list = []\n",
    "    \n",
    "    # Получаем количество образцов\n",
    "    num_samples = z2.shape[0]\n",
    "    \n",
    "    # Проходим по всем образцам\n",
    "    for sample_index in range(num_samples):\n",
    "        # Получаем вектор для текущего образца\n",
    "        current_vector = z2[sample_index]\n",
    "        \n",
    "        # Применяем softmax к текущему вектору\n",
    "        softmax_result = softmax_activation(current_vector)\n",
    "        \n",
    "        # Добавляем результат в список\n",
    "        a2_list.append(softmax_result)\n",
    "    \n",
    "    # Преобразуем список в массив numpy\n",
    "    a2 = np.array(a2_list)\n",
    "    \n",
    "    # Возвращаем все промежуточные значения\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# Функция обратного распространения для двухслойной сети\n",
    "def two_layer_backward_propagation(input_data, true_labels, z1, a1, z2, a2, weights1, biases1, weights2, biases2, learning_rate):\n",
    "    # Получаем размер батча (сколько изображений обрабатываем за раз)\n",
    "    batch_size = input_data.shape[0]\n",
    "    \n",
    "    # Получаем размерности для удобства\n",
    "    num_input_features = input_data.shape[1]  # 784 пикселя\n",
    "    num_hidden_neurons = weights1.shape[1]    # 25 нейронов в скрытом слое\n",
    "    num_output_neurons = weights2.shape[1]    # 10 нейронов в выходном слое\n",
    "    \n",
    "    # Градиенты для выходного слоя (второго слоя)\n",
    "    # dz2 = ошибка выходного слоя: предсказание минус правильный ответ\n",
    "    dz2 = a2 - true_labels\n",
    "    \n",
    "    # Градиенты весов второго слоя (dw2) \n",
    "    dw2 = np.zeros((num_hidden_neurons, num_output_neurons))\n",
    "    \n",
    "    # Проходим по всем образцам в батче\n",
    "    for i in range(batch_size):\n",
    "        # Получаем активации скрытого слоя для i-го образца (25 чисел)\n",
    "        current_a1 = a1[i]\n",
    "        \n",
    "        # Получаем градиент выходного слоя для i-го образца (10 чисел)\n",
    "        current_dz2 = dz2[i]\n",
    "        \n",
    "        # Для каждой пары (скрытый нейрон, выходной нейрон) вычисляем градиент\n",
    "        for hidden_idx in range(num_hidden_neurons):\n",
    "            for output_idx in range(num_output_neurons):\n",
    "                # Градиент = активация скрытого нейрона × градиент выходного нейрона\n",
    "                # Это реализация формулы: dW2[j,k] += a1[i,j] * dz2[i,k]\n",
    "                dw2[hidden_idx, output_idx] += current_a1[hidden_idx] * current_dz2[output_idx]\n",
    "    \n",
    "    # Усредняем градиенты по размеру батча\n",
    "    dw2 = dw2 / batch_size\n",
    "    \n",
    "    # Градиенты смещений второго слоя (db2)\n",
    "    # db2 должен иметь размер (10,) - как biases2\n",
    "    db2 = np.zeros(num_output_neurons)\n",
    "    \n",
    "    # Проходим по всем образцам в батче\n",
    "    for i in range(batch_size):\n",
    "        current_dz2 = dz2[i]\n",
    "        for output_idx in range(num_output_neurons):\n",
    "            # Градиент смещения = градиент выходного нейрона\n",
    "            # Это реализация формулы: db2[k] += dz2[i,k]\n",
    "            db2[output_idx] += current_dz2[output_idx]\n",
    "    \n",
    "    # Усредняем\n",
    "    db2 = db2 / batch_size\n",
    "    \n",
    "    # Градиенты для скрытого слоя (первого слоя)\n",
    "    # dz1 = градиент для активаций скрытого слоя\n",
    "    dz1 = np.zeros((batch_size, num_hidden_neurons))\n",
    "    \n",
    "    # Проходим по всем образцам\n",
    "    for i in range(batch_size):\n",
    "        current_dz2 = dz2[i]  # Градиенты выходного слоя для i-го образца\n",
    "        current_a1 = a1[i]    # Активации скрытого слоя для i-го образца\n",
    "        \n",
    "        # Для каждого нейрона скрытого слоя\n",
    "        for hidden_idx in range(num_hidden_neurons):\n",
    "            # Вычисляем сумму: dz2 × веса второго слоя\n",
    "            # Это часть формулы: dz1 = dz2 × W2^T\n",
    "            sum_for_neuron = 0.0\n",
    "            for output_idx in range(num_output_neurons):\n",
    "                # Берём градиент выходного нейрона и умножаем на соответствующий вес\n",
    "                # weights2[hidden_idx, output_idx] - вес от hidden_idx к output_idx\n",
    "                sum_for_neuron += current_dz2[output_idx] * weights2[hidden_idx, output_idx]\n",
    "            \n",
    "            # Умножаем на производную ReLU от активации скрытого нейрона\n",
    "            # relu_derivative возвращает 1 если current_a1[hidden_idx] > 0, иначе 0\n",
    "            dz1[i, hidden_idx] = sum_for_neuron * relu_derivative(current_a1[hidden_idx])\n",
    "    \n",
    "    # Градиенты весов первого слоя\n",
    "    dw1 = np.zeros((num_input_features, num_hidden_neurons))\n",
    "    \n",
    "    # Проходим по всем образцам\n",
    "    for i in range(batch_size):\n",
    "        current_input = input_data[i]  # Входные данные i-го образца (784 пикселя)\n",
    "        current_dz1 = dz1[i]           # Градиент скрытого слоя для i-го образца\n",
    "        \n",
    "        # Для каждой пары (входной признак, скрытый нейрон)\n",
    "        for feature_idx in range(num_input_features):\n",
    "            for hidden_idx in range(num_hidden_neurons):\n",
    "                # Градиент = входной признак × градиент скрытого нейрона\n",
    "                # Это реализация формулы: dW1[j,k] += input[i,j] * dz1[i,k]\n",
    "                dw1[feature_idx, hidden_idx] += current_input[feature_idx] * current_dz1[hidden_idx]\n",
    "    \n",
    "    # Усредняем\n",
    "    dw1 = dw1 / batch_size\n",
    "    \n",
    "    # Градиенты смещений первого слоя (db1)\n",
    "    db1 = np.zeros(num_hidden_neurons)\n",
    "    \n",
    "    # Проходим по всем образцам\n",
    "    for i in range(batch_size):\n",
    "        current_dz1 = dz1[i]\n",
    "        for hidden_idx in range(num_hidden_neurons):\n",
    "            # Градиент смещения = градиент скрытого нейрона\n",
    "            db1[hidden_idx] += current_dz1[hidden_idx]\n",
    "    \n",
    "    # Усредняем\n",
    "    db1 = db1 / batch_size\n",
    "    \n",
    "    # Обновление параметров второго слоя\n",
    "    # Формула градиентного спуска: параметр = параметр - скорость_обучения × градиент\n",
    "    for hidden_idx in range(num_hidden_neurons):\n",
    "        for output_idx in range(num_output_neurons):\n",
    "            weights2[hidden_idx, output_idx] = weights2[hidden_idx, output_idx] - (learning_rate * dw2[hidden_idx, output_idx])\n",
    "    \n",
    "    for output_idx in range(num_output_neurons):\n",
    "        biases2[output_idx] = biases2[output_idx] - (learning_rate * db2[output_idx])\n",
    "    \n",
    "    # Обновление параметров первого слоя\n",
    "    for feature_idx in range(num_input_features):\n",
    "        for hidden_idx in range(num_hidden_neurons):\n",
    "            weights1[feature_idx, hidden_idx] = weights1[feature_idx, hidden_idx] - (learning_rate * dw1[feature_idx, hidden_idx])\n",
    "    \n",
    "    for hidden_idx in range(num_hidden_neurons):\n",
    "        biases1[hidden_idx] = biases1[hidden_idx] - (learning_rate * db1[hidden_idx])\n",
    "    \n",
    "    # Возвращаем обновленные параметры\n",
    "    return weights1, biases1, weights2, biases2\n",
    "\n",
    "# Функция для обучения двухслойной сети\n",
    "def train_two_layer_network(training_data, training_labels, input_size, hidden_size, output_size, learning_rate=0.1, epochs=500):\n",
    "    # Инициализируем параметры сети\n",
    "    weights1, biases1, weights2, biases2 = initialize_two_layer_parameters(\n",
    "        input_size, hidden_size, output_size\n",
    "    )\n",
    "    \n",
    "    # Преобразуем метки в one-hot encoding\n",
    "    one_hot_labels = convert_to_one_hot(training_labels)\n",
    "    \n",
    "    # Получаем количество образцов\n",
    "    num_samples = len(training_data)\n",
    "    \n",
    "    # Цикл обучения по эпохам\n",
    "    for epoch in range(epochs):\n",
    "        # Прямой проход через сеть\n",
    "        z1, a1, z2, a2 = two_layer_forward_propagation(\n",
    "            training_data, weights1, biases1, weights2, biases2\n",
    "        )\n",
    "        \n",
    "        # Обратное распространение и обновление параметров\n",
    "        weights1, biases1, weights2, biases2 = two_layer_backward_propagation(\n",
    "            training_data, one_hot_labels, z1, a1, z2, a2,\n",
    "            weights1, biases1, weights2, biases2, learning_rate\n",
    "        )\n",
    "        \n",
    "        # Вывод прогресса обучения каждые 100 эпох\n",
    "        if epoch % 100 == 0:\n",
    "            # Инициализируем переменную для общей потери\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            # Вычисляем кросс-энтропийную потерю\n",
    "            for i in range(num_samples):\n",
    "                # Получаем истинный класс текущего образца\n",
    "                true_class = training_labels[i]\n",
    "                \n",
    "                # Получаем вероятность правильного класса\n",
    "                correct_probability = a2[i, true_class]\n",
    "                \n",
    "                # Защита от нулевой вероятности\n",
    "                if correct_probability < 1e-10:\n",
    "                    correct_probability = 1e-10\n",
    "                \n",
    "                # Вычисляем логарифм вероятности\n",
    "                log_prob = math.log(correct_probability)\n",
    "                \n",
    "                # Добавляем к общей потере\n",
    "                total_loss = total_loss - log_prob\n",
    "            \n",
    "            # Вычисляем среднюю потерю\n",
    "            average_loss = total_loss / num_samples\n",
    "            \n",
    "            # Выводим информацию о текущей эпохе и потере\n",
    "            print(f\"Эпоха {epoch}, Средние потери: {average_loss:.4f}\")\n",
    "    \n",
    "    # Возвращаем обученные параметры\n",
    "    return weights1, biases1, weights2, biases2\n",
    "\n",
    "# Функция для предсказания с использованием двухслойной сети\n",
    "def predict_two_layer(input_data, weights1, biases1, weights2, biases2):\n",
    "    # Выполняем прямой проход через сеть\n",
    "    z1, a1, z2, a2 = two_layer_forward_propagation(\n",
    "        input_data, weights1, biases1, weights2, biases2\n",
    "    )\n",
    "    \n",
    "    # Создаем пустой список для предсказанных меток\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # Получаем количество образцов\n",
    "    num_samples = a2.shape[0]\n",
    "    \n",
    "    # Проходим по всем образцам\n",
    "    for sample_index in range(num_samples):\n",
    "        # Получаем вероятности для текущего образца\n",
    "        current_probs = a2[sample_index]\n",
    "        \n",
    "        # Инициализируем лучшую метку и максимальную вероятность\n",
    "        best_label = 0\n",
    "        max_prob = current_probs[0]\n",
    "        \n",
    "        # Проходим по всем классам, начиная со второго\n",
    "        for label_index in range(1, len(current_probs)):\n",
    "            # Сравниваем текущую вероятность с максимальной\n",
    "            if current_probs[label_index] > max_prob:\n",
    "                # Обновляем максимальную вероятность и лучшую метку\n",
    "                max_prob = current_probs[label_index]\n",
    "                best_label = label_index\n",
    "        \n",
    "        # Добавляем предсказанную метку в список\n",
    "        predicted_labels.append(best_label)\n",
    "    \n",
    "    # Преобразуем список в массив numpy\n",
    "    return np.array(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d36381-8ada-4f78-9851-ee185c375d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало обучения однослойной нейронной сети:\n",
      "Эпоха 0, Средняя ошибка: 0.2518\n"
     ]
    }
   ],
   "source": [
    "# Обучение однослойной сети\n",
    "\n",
    "# Выводим сообщение о начале обучения\n",
    "print(\"Начало обучения однослойной нейронной сети:\")\n",
    "\n",
    "# Получаем размерность входных данных\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Определяем количество выходных нейронов (10 цифр)\n",
    "output_size = 10\n",
    "\n",
    "# Обучаем однослойную сеть\n",
    "single_layer_weights, single_layer_biases = train_single_layer_network(\n",
    "    training_data=X_train,\n",
    "    training_labels=y_train,\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    learning_rate=0.1,\n",
    "    epochs=500\n",
    ")\n",
    "\n",
    "# Выводим сообщение об окончании обучения\n",
    "print(\"Обучение однослойной сети завершено\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80103763-7b7e-49c6-8453-cac08e6a2460",
   "metadata": {},
   "source": [
    "**Алгоритм обучения:**\n",
    "\n",
    "Для каждой эпохи (500 раз):\n",
    "  1. Прямой проход:\n",
    "     - Z1 = вход × Веса1 + Смещения1\n",
    "     - A1 = ReLU(Z1)              ← 25 признаков!\n",
    "     - Z2 = A1 × Веса2 + Смещения2\n",
    "     - A2 = Softmax(Z2)           ← 10 вероятностей!\n",
    "   \n",
    "  2. Вычисление ошибки:\n",
    "     - Ошибка = A2 - One-hot метка\n",
    "   \n",
    "  3. Обратное распространение: (найти градиенты)\n",
    "     - dZ2 = ошибка\n",
    "     - dW2 = A1ᵀ × dZ2 / batch_size\n",
    "     - db2 = сумма(dZ2) / batch_size\n",
    "     - dZ1 = (dZ2 × Веса2ᵀ) × производная_ReLU(A1)\n",
    "     - dW1 = входᵀ × dZ1 / batch_size\n",
    "     - db1 = сумма(dZ1) / batch_size\n",
    "   \n",
    "  4. Обновление весов: (градиентный спуск)\n",
    "     - Веса2 = Веса2 - 0.1 × dW2\n",
    "     - Смещения2 = Смещения2 - 0.1 × db2\n",
    "     - Веса1 = Веса1 - 0.1 × dW1\n",
    "     - Смещения1 = Смещения1 - 0.1 × db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1c88f-1b9b-41ec-96c1-263c24a0b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение двухслойной сети\n",
    "\n",
    "# Выводим сообщение о начале обучения\n",
    "print(\"Начало обучения двухслойной нейронной сети:\")\n",
    "\n",
    "# Получаем размерность входных данных\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Определяем количество нейронов в скрытом слое (25 как в задании)\n",
    "hidden_size = 25\n",
    "\n",
    "# Определяем количество выходных нейронов (10 цифр)\n",
    "output_size = 10\n",
    "\n",
    "# Обучаем двухслойную сеть\n",
    "two_layer_weights1, two_layer_biases1, two_layer_weights2, two_layer_biases2 = train_two_layer_network(\n",
    "    training_data=X_train,\n",
    "    training_labels=y_train,\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    learning_rate=0.1,\n",
    "    epochs=500\n",
    ")\n",
    "\n",
    "# Выводим сообщение об окончании обучения\n",
    "print(\"Обучение двухслойной сети завершено\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c7d8f-892b-4bea-aca6-4ae799596268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка точности и сравнение моделей\n",
    "\n",
    "# Функция для вычисления точности предсказаний\n",
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    # Инициализируем счетчик правильных предсказаний\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    # Получаем общее количество предсказаний\n",
    "    total_predictions = len(true_labels)\n",
    "    \n",
    "    # Проходим по всем образцам\n",
    "    for i in range(total_predictions):\n",
    "        # Сравниваем истинную метку с предсказанной\n",
    "        if true_labels[i] == predicted_labels[i]:\n",
    "            # Если совпадают, увеличиваем счетчик\n",
    "            correct_predictions = correct_predictions + 1\n",
    "    \n",
    "    # Вычисляем точность как отношение правильных к общему количеству\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    # Возвращаем точность\n",
    "    return accuracy\n",
    "\n",
    "# Выводим заголовок для оценки точности\n",
    "print(\"Оценка точности моделей на тестовой выборке:\")\n",
    "\n",
    "# Получаем предсказания однослойной сети на тестовых данных\n",
    "y_pred_single = predict_single_layer(\n",
    "    input_data=X_test,\n",
    "    weights=single_layer_weights,\n",
    "    biases=single_layer_biases\n",
    ")\n",
    "\n",
    "# Вычисляем точность однослойной сети\n",
    "accuracy_single = calculate_accuracy(y_test, y_pred_single)\n",
    "\n",
    "# Выводим точность однослойной сети с округлением до 4 знаков после запятой\n",
    "print(f\"Точность однослойной сети: {accuracy_single:.4f}\")\n",
    "\n",
    "# Получаем предсказания двухслойной сети на тестовых данных\n",
    "y_pred_two = predict_two_layer(\n",
    "    input_data=X_test,\n",
    "    weights1=two_layer_weights1,\n",
    "    biases1=two_layer_biases1,\n",
    "    weights2=two_layer_weights2,\n",
    "    biases2=two_layer_biases2\n",
    ")\n",
    "\n",
    "# Вычисляем точность двухслойной сети\n",
    "accuracy_two = calculate_accuracy(y_test, y_pred_two)\n",
    "\n",
    "# Выводим точность двухслойной сети с округлением до 4 знаков после запятой\n",
    "print(f\"Точность двухслойной сети: {accuracy_two:.4f}\")\n",
    "\n",
    "# Выводим заголовок для сравнения результатов\n",
    "print(\"\\nСравнение результатов:\")\n",
    "\n",
    "# Сравниваем точности двух моделей\n",
    "if accuracy_two > accuracy_single:\n",
    "    # Если двухслойная сеть лучше, вычисляем процент улучшения\n",
    "    improvement = ((accuracy_two - accuracy_single) / accuracy_single) * 100\n",
    "    # Выводим результат сравнения\n",
    "    print(f\"Двухслойная сеть лучше на {improvement:.2f}%\")\n",
    "elif accuracy_single > accuracy_two:\n",
    "    # Если однослойная сеть лучше, вычисляем процент улучшения\n",
    "    improvement = ((accuracy_single - accuracy_two) / accuracy_two) * 100\n",
    "    # Выводим результат сравнения\n",
    "    print(f\"Однослойная сеть лучше на {improvement:.2f}%\")\n",
    "else:\n",
    "    # Если точности равны, выводим соответствующее сообщение\n",
    "    print(\"Обе сети показали одинаковую точность\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c3dc9-2e56-4d7c-a265-37e28881c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация примеров и анализ ошибок\n",
    "\n",
    "# Функция для визуализации примеров предсказаний\n",
    "def visualize_predictions(features, true_labels, predictions, num_examples=5):\n",
    "    # Выводим заголовок с количеством примеров\n",
    "    print(f\"\\nВизуализация {num_examples} случайных примеров:\")\n",
    "    \n",
    "    # Проходим по заданному количеству примеров\n",
    "    for example in range(num_examples):\n",
    "        # Генерируем случайный индекс из тестовой выборки\n",
    "        random_index = random.randint(0, len(features) - 1)\n",
    "        \n",
    "        # Получаем истинную цифру для выбранного индекса\n",
    "        true_digit = true_labels[random_index]\n",
    "        \n",
    "        # Получаем предсказанную цифру для выбранного индекса\n",
    "        predicted_digit = predictions[random_index]\n",
    "        \n",
    "        # Определяем, правильно ли предсказана цифра\n",
    "        if true_digit == predicted_digit:\n",
    "            result = \"Правильно\"\n",
    "        else:\n",
    "            result = \"Ошибка\"\n",
    "        \n",
    "        # Выводим информацию о примере\n",
    "        print(f\"Пример {example + 1}: Истинная цифра = {true_digit}, \" +\n",
    "              f\"Предсказанная цифра = {predicted_digit} {result}\")\n",
    "\n",
    "# Визуализируем примеры для однослойной сети\n",
    "print(\"Однослойная сеть:\")\n",
    "visualize_predictions(X_test, y_test, y_pred_single)\n",
    "\n",
    "# Визуализируем примеры для двухслойной сети\n",
    "print(\"\\nДвухслойная сеть:\")\n",
    "visualize_predictions(X_test, y_test, y_pred_two)\n",
    "\n",
    "# Функция для анализа ошибок по классам\n",
    "def analyze_errors(true_labels, predictions):\n",
    "    # Создаем пустой словарь для анализа ошибок\n",
    "    error_analysis = {}\n",
    "    \n",
    "    # Получаем количество образцов\n",
    "    num_samples = len(true_labels)\n",
    "    \n",
    "    # Проходим по всем образцам\n",
    "    for i in range(num_samples):\n",
    "        # Получаем истинную и предсказанную метки\n",
    "        true = true_labels[i]\n",
    "        pred = predictions[i]\n",
    "        \n",
    "        # Проверяем, является ли предсказание ошибочным\n",
    "        if true != pred:\n",
    "            # Создаем ключ из пары (истинная цифра, ошибочное предсказание)\n",
    "            key = (true, pred)\n",
    "            \n",
    "            # Проверяем, есть ли уже такая ошибка в словаре\n",
    "            if key in error_analysis:\n",
    "                # Если есть, увеличиваем счетчик\n",
    "                error_analysis[key] = error_analysis[key] + 1\n",
    "            else:\n",
    "                # Если нет, добавляем новую запись\n",
    "                error_analysis[key] = 1\n",
    "    \n",
    "    # Выводим заголовок для анализа ошибок\n",
    "    print(\"\\nАнализ наиболее частых ошибок:\")\n",
    "    \n",
    "    # Преобразуем словарь в список кортежей для сортировки\n",
    "    error_items = []\n",
    "    for key, value in error_analysis.items():\n",
    "        error_items.append((key, value))\n",
    "    \n",
    "    # Сортируем ошибки по количеству в убывающем порядке\n",
    "    # Используем ключевую функцию, которая возвращает второй элемент кортежа (количество)\n",
    "    def get_count(item):\n",
    "        return item[1]\n",
    "    \n",
    "    sorted_errors = sorted(error_items, key=get_count, reverse=True)\n",
    "    \n",
    "    # Выводим 5 самых частых ошибок\n",
    "    for i in range(min(5, len(sorted_errors))):\n",
    "        error_pair, count = sorted_errors[i]\n",
    "        true_digit, wrong_digit = error_pair\n",
    "        print(f\"Цифра {true_digit} ошибочно предсказана как {wrong_digit}: {count} раз\")\n",
    "\n",
    "# Анализируем ошибки однослойной сети\n",
    "print(\"Ошибки однослойной сети:\")\n",
    "analyze_errors(y_test, y_pred_single)\n",
    "\n",
    "# Анализируем ошибки двухслойной сети\n",
    "print(\"\\nОшибки двухслойной сети:\")\n",
    "analyze_errors(y_test, y_pred_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3f4a7-040b-403e-8f6b-2b2c334fc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводы\n",
    "\n",
    "# Выводим разделительную линию\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Выводим заголовок заключения\n",
    "print(\"ЗАКЛЮЧЕНИЕ И ВЫВОДЫ\")\n",
    "\n",
    "# Выводим разделительную линию\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Выводим результаты экспериментов\n",
    "print(\"Результаты экспериментов с нейронными сетями:\")\n",
    "\n",
    "# Выводим точность однослойной сети в процентах\n",
    "print(f\"1. Однослойная сеть (сигмоида): {accuracy_single*100:.2f}% точности\")\n",
    "\n",
    "# Выводим точность двухслойной сети в процентах\n",
    "print(f\"2. Двухслойная сеть (ReLU + Softmax): {accuracy_two*100:.2f}% точности\")\n",
    "\n",
    "# Анализируем, какая сеть показала лучший результат\n",
    "if accuracy_two > accuracy_single:\n",
    "    # Вычисляем абсолютное улучшение точности\n",
    "    improvement = accuracy_two - accuracy_single\n",
    "    \n",
    "    # Выводим информацию о том, что двухслойная сеть лучше\n",
    "    print(f\"\\nДвухслойная сеть показала лучший результат.\")\n",
    "    \n",
    "    # Выводим процентное улучшение точности\n",
    "    print(f\"Улучшение точности: {improvement*100:.2f}%\")\n",
    "    \n",
    "    # Объясняем причину улучшения\n",
    "    print(\"Это объясняется более сложной архитектурой сети,\")\n",
    "    print(\"которая может обучаться более сложным признакам.\")\n",
    "    \n",
    "elif accuracy_single > accuracy_two:\n",
    "    # Вычисляем абсолютное улучшение точности\n",
    "    improvement = accuracy_single - accuracy_two\n",
    "    \n",
    "    # Выводим информацию о том, что однослойная сеть лучше\n",
    "    print(f\"\\nОднослойная сеть показала лучший результат.\")\n",
    "    \n",
    "    # Выводим процентное улучшение точности\n",
    "    print(f\"Улучшение точности: {improvement*100:.2f}%\")\n",
    "    \n",
    "    # Объясняем возможные причины\n",
    "    print(\"Это может быть связано с особенностями датасета\")\n",
    "    print(\"или параметрами обучения.\")\n",
    "    \n",
    "else:\n",
    "    # Выводим сообщение, если точности равны\n",
    "    print(f\"\\nОбе сети показали одинаковую точность.\")\n",
    "\n",
    "# Выводим общие выводы\n",
    "print(\"\\nОбщие выводы:\")\n",
    "\n",
    "# Вывод 1: обе сети способны обучаться\n",
    "print(\"- Обе сети способны обучаться распознаванию рукописных цифр\")\n",
    "\n",
    "# Вывод 2: добавление скрытого слоя может улучшить результаты\n",
    "print(\"- Добавление скрытого слоя может улучшить качество классификации\")\n",
    "\n",
    "# Вывод 3: выбор архитектуры зависит от сложности задачи\n",
    "print(\"- Выбор архитектуры сети зависит от сложности задачи\")\n",
    "\n",
    "# Вывод 4: для сложных задач нужны более глубокие сети\n",
    "print(\"- Для более сложных задач обычно требуются более глубокие сети\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
