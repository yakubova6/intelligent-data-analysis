{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73df418-db47-4d44-a84f-21debeee86f7",
   "metadata": {},
   "source": [
    "### Задание на ПР-7:\n",
    "В приложенном датасете приведены данные о реальных больных сердечно-сосудистыми заболеваниями. Последняя компонента в каждом экземпляре - бинарная метка (0 - нет болезни, 1 - есть болезнь). Ваша задача - построить классификатор, который на данных нового пациента предскажет наличие у него болезни. Затем нужно вычислить метрики качества вашего классификатора. \n",
    "1. Сделайте препроцессинг.\n",
    "2. Разделите данные на обучающие и тестовые (70/30), причем проследите чтобы пропорции 0 и 1 в них были примерно одинаковыми.\n",
    "3. Напишите функцию, которая вычислит коэффициенты логистической регрессии на обучающей выборке.\n",
    "4. Напишите функцию, которая предскажет метку на тестовом экземпляре данных, пользуясь вычисленными коэффициентами логистической регрессии (ЛР).\n",
    "5. Выберите из датасета такие экземпляры, на которых логистическая регрессия дает нестабильные предсказания. Таких экземпляров должно быть не менее 20% от всего датасета. Отметьте их третьим типом метки (например, -1, или 2 - как вам нравится больше).\n",
    "6. Постройте классификацию (выполните пп. 3 и 4) методом ближайших соседей (БС). \n",
    "7. Измерьте качество классификации для ЛР и БС по метрикам Precision, Recall, F1 (для бинарной классификации) и Accuracy (для тернарной классификации). \n",
    "8. Интерпретируйте результаты - сделайте выводы о качестве классификации, проведенной на основе разных методов и с разным числом классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e8b667-8ab3-4ce7-b732-0e5104ace008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт библиотек и базовые функции\n",
    "import math  # математические функции\n",
    "import random  # функции для генерации случайных чисел\n",
    "import csv  # функции для работы с CSV файлами\n",
    "\n",
    "# Функция для скалярного произведения двух векторов\n",
    "def dot_product(vector1, vector2):\n",
    "    \"\"\"Вычисляет скалярное произведение двух векторов одинаковой длины\"\"\"\n",
    "    result = 0.0  # Инициализируем переменную для результата\n",
    "    # Проходим по всем элементам векторов\n",
    "    for element_index in range(len(vector1)):\n",
    "        # Умножаем соответствующие элементы и добавляем к результату\n",
    "        result = result + vector1[element_index] * vector2[element_index]\n",
    "    return result  # Возвращаем скалярное произведение\n",
    "\n",
    "# Функция для сложения двух векторов\n",
    "def vector_add(vector1, vector2):\n",
    "    \"\"\"Поэлементное сложение двух векторов одинаковой длины\"\"\"\n",
    "    result_vector = []  # Создаем пустой список для результата\n",
    "    # Проходим по всем элементам векторов\n",
    "    for element_index in range(len(vector1)):\n",
    "        # Складываем соответствующие элементы и добавляем в результат\n",
    "        result_vector.append(vector1[element_index] + vector2[element_index])\n",
    "    return result_vector  # Возвращаем результирующий вектор\n",
    "\n",
    "# Функция для умножения вектора на скаляр\n",
    "def scalar_multiply(scalar, vector):\n",
    "    \"\"\"Умножает каждый элемент вектора на скаляр\"\"\"\n",
    "    result_vector = []  # Создаем пустой список для результата\n",
    "    # Проходим по всем элементам вектора\n",
    "    for element_index in range(len(vector)):\n",
    "        # Умножаем элемент на скаляр и добавляем в результат\n",
    "        result_vector.append(scalar * vector[element_index])\n",
    "    return result_vector  # Возвращаем результирующий вектор\n",
    "\n",
    "# Функция для вычисления градиента\n",
    "def gradient(function_to_diff, point, step_size=1e-05):\n",
    "    \"\"\"Вычисляет градиент функции в заданной точке с использованием конечных разностей\"\"\"\n",
    "    gradient_vector = []  # Создаем пустой список для градиента\n",
    "    # Проходим по всем координатам точки\n",
    "    for coordinate_index in range(len(point)):\n",
    "        perturbed_point = []  # Создаем возмущенную точку\n",
    "        # Создаем копию точки с возмущением по текущей координате\n",
    "        for coordinate_index_inner in range(len(point)):\n",
    "            if coordinate_index_inner == coordinate_index:\n",
    "                # Добавляем шаг к текущей координате\n",
    "                perturbed_point.append(point[coordinate_index_inner] + step_size)\n",
    "            else:\n",
    "                # Оставляем координату без изменений\n",
    "                perturbed_point.append(point[coordinate_index_inner])\n",
    "        # Вычисляем частную производную как разностное отношение\n",
    "        partial_derivative = (function_to_diff(perturbed_point) - function_to_diff(point)) / step_size\n",
    "        gradient_vector.append(partial_derivative)  # Добавляем в градиент\n",
    "    return gradient_vector  # Возвращаем вектор градиента\n",
    "\n",
    "# Функция градиентного спуска\n",
    "def gradient_descent(function_to_minimize, initial_point, learning_rate=0.05, step_size=1e-05, max_iterations=5000):\n",
    "    \"\"\"Реализация градиентного спуска для минимизации функции\"\"\"\n",
    "    current_point = []  # Создаем список для текущей точки\n",
    "    # Копируем начальную точку\n",
    "    for value in initial_point:\n",
    "        current_point.append(value)\n",
    "    \n",
    "    # Выполняем итерации градиентного спуска\n",
    "    for iteration in range(max_iterations):\n",
    "        # Вычисляем градиент в текущей точке\n",
    "        current_gradient = gradient(function_to_minimize, current_point, step_size)\n",
    "        # Вычисляем норму градиента\n",
    "        gradient_norm = 0.0\n",
    "        for gradient_component in current_gradient:\n",
    "            gradient_norm = gradient_norm + gradient_component * gradient_component\n",
    "        gradient_norm = math.sqrt(gradient_nradient_norm)\n",
    "        # Масштабируем градиент на скорость обучения\n",
    "        scaled_gradient = scalar_multiply(learning_rate * gradient_norm, current_gradient)\n",
    "        # Делаем шаг в направлении, противоположном градиенту\n",
    "        current_point = vector_add(current_point, scalar_multiply(-1, scaled_gradient))\n",
    "    return current_point  # Возвращаем найденную точку минимума"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa485b36-c93d-4546-969a-ff574f2026b3",
   "metadata": {},
   "source": [
    "#### ЗАДАНИЕ 1: ПРЕПРОЦЕССИНГ ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30830393-96d5-49c8-986f-c292f05d9392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 918 записей\n"
     ]
    }
   ],
   "source": [
    "# Загрузка и препроцессинг данных\n",
    "def load_data(filename):\n",
    "    \"\"\"Загружает данные из CSV файла и возвращает список словарей\"\"\"\n",
    "    data_list = []  # Создаем пустой список для данных\n",
    "    # Открываем файл для чтения\n",
    "    with open(filename, 'r') as file_handle:\n",
    "        # Создаем CSV reader для чтения данных\n",
    "        csv_reader = csv.DictReader(file_handle)\n",
    "        # Читаем каждую строку файла\n",
    "        for row_dict in csv_reader:\n",
    "            data_list.append(row_dict)  # Добавляем строку в список данных\n",
    "    return data_list  # Возвращаем загруженные данные\n",
    "\n",
    "def preprocess_data(raw_data):\n",
    "    \"\"\"Преобразует категориальные признаки в числовые значения\"\"\"\n",
    "    processed_data_list = []  # Создаем пустой список для обработанных данных\n",
    "    \n",
    "    # Создаем словари для преобразования категориальных признаков\n",
    "    gender_mapping = {'M': 1, 'F': 0}  # Мужской пол -> 1, женский -> 0\n",
    "    chest_pain_mapping = {'ATA': 0, 'NAP': 1, 'ASY': 2, 'TA': 3}  # Типы боли в груди\n",
    "    ecg_mapping = {'Normal': 0, 'ST': 1, 'LVH': 2}  # Типы ЭКГ\n",
    "    angina_mapping = {'Y': 1, 'N': 0}  # Наличие стенокардии при нагрузке\n",
    "    slope_mapping = {'Up': 0, 'Flat': 1, 'Down': 2}  # Наклон сегмента ST\n",
    "    \n",
    "    # Обрабатываем каждую строку исходных данных\n",
    "    for data_row in raw_data:\n",
    "        processed_row = []  # Создаем пустой список для обработанной строки\n",
    "        # Преобразуем возраст в целое число\n",
    "        processed_row.append(int(data_row['Age']))\n",
    "        # Преобразуем пол с помощью словаря\n",
    "        processed_row.append(gender_mapping[data_row['Sex']])\n",
    "        # Преобразуем тип боли в груди\n",
    "        processed_row.append(chest_pain_mapping[data_row['ChestPainType']])\n",
    "        # Преобразуем артериальное давление в покое\n",
    "        processed_row.append(int(data_row['RestingBP']))\n",
    "        # Преобразуем уровень холестерина\n",
    "        processed_row.append(int(data_row['Cholesterol']))\n",
    "        # Преобразуем уровень сахара натощак\n",
    "        processed_row.append(int(data_row['FastingBS']))\n",
    "        # Преобразуем результат ЭКГ\n",
    "        processed_row.append(ecg_mapping[data_row['RestingECG']])\n",
    "        # Преобразуем максимальный пульс\n",
    "        processed_row.append(int(data_row['MaxHR']))\n",
    "        # Преобразуем наличие стенокардии при нагрузке\n",
    "        processed_row.append(angina_mapping[data_row['ExerciseAngina']])\n",
    "        # Преобразуем депрессию ST сегмента в вещественное число\n",
    "        processed_row.append(float(data_row['Oldpeak']))\n",
    "        # Преобразуем наклон ST сегмента\n",
    "        processed_row.append(slope_mapping[data_row['ST_Slope']])\n",
    "        # Преобразуем целевую переменную (наличие заболевания)\n",
    "        processed_row.append(int(data_row['HeartDisease']))\n",
    "        # Добавляем обработанную строку в список\n",
    "        processed_data_list.append(processed_row)\n",
    "    return processed_data_list  # Возвращаем обработанные данные\n",
    "\n",
    "# Загружаем исходные данные из CSV файла\n",
    "raw_dataset = load_data('heart.csv')\n",
    "# Преобразуем категориальные признаки в числовые\n",
    "processed_dataset = preprocess_data(raw_dataset)\n",
    "# Выводим информацию о количестве загруженных записей\n",
    "print(\"Загружено \" + str(len(processed_dataset)) + \" записей\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc88c970-748d-442d-8277-8cebfb60bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Добавлен intercept, размерность данных: 13\n"
     ]
    }
   ],
   "source": [
    "# Масштабирование признаков и добавление intercept\n",
    "def scale_features(input_data):\n",
    "    \"\"\"Масштабирует признаки методом стандартизации (z-score normalization)\"\"\"\n",
    "    # Проверяем, что данные не пустые\n",
    "    if len(input_data) == 0:\n",
    "        return input_data\n",
    "    \n",
    "    # Транспонируем данные для обработки по столбцам (признакам)\n",
    "    transposed_data = []  # Создаем список для транспонированных данных\n",
    "    # Проходим по всем столбцам (признакам)\n",
    "    for column_index in range(len(input_data[0])):\n",
    "        current_column = []  # Создаем список для текущего столбца\n",
    "        # Собираем все значения текущего столбца\n",
    "        for row_index in range(len(input_data)):\n",
    "            current_column.append(input_data[row_index][column_index])\n",
    "        transposed_data.append(current_column)  # Добавляем столбец в транспонированные данные\n",
    "    \n",
    "    scaled_transposed_data = []  # Создаем список для масштабированных данных\n",
    "    \n",
    "    # Обрабатываем все признаки кроме последнего (целевой переменной)\n",
    "    for feature_index in range(len(transposed_data) - 1):\n",
    "        feature_column = transposed_data[feature_index]  # Получаем текущий признак\n",
    "        # Вычисляем среднее значение признака\n",
    "        feature_sum = 0.0\n",
    "        for feature_value in feature_column:\n",
    "            feature_sum = feature_sum + feature_value\n",
    "        feature_mean = feature_sum / len(feature_column)\n",
    "        \n",
    "        # Вычисляем стандартное отклонение признака\n",
    "        sum_squared_diff = 0.0\n",
    "        for feature_value in feature_column:\n",
    "            sum_squared_diff = sum_squared_diff + (feature_value - feature_mean) * (feature_value - feature_mean)\n",
    "        feature_std = math.sqrt(sum_squared_diff / len(feature_column))\n",
    "        \n",
    "        # Если стандартное отклонение равно 0, устанавливаем 1\n",
    "        if feature_std == 0:\n",
    "            feature_std = 1\n",
    "        \n",
    "        # Масштабируем значения признака\n",
    "        scaled_feature_column = []\n",
    "        for feature_value in feature_column:\n",
    "            scaled_value = (feature_value - feature_mean) / feature_std\n",
    "            scaled_feature_column.append(scaled_value)\n",
    "        scaled_transposed_data.append(scaled_feature_column)\n",
    "    \n",
    "    # Добавляем целевую переменную без изменений\n",
    "    scaled_transposed_data.append(transposed_data[-1])\n",
    "    \n",
    "    # Возвращаем данные к исходной структуре (транспонируем обратно)\n",
    "    scaled_dataset = []\n",
    "    for row_index in range(len(scaled_transposed_data[0])):\n",
    "        new_row = []\n",
    "        for column_index in range(len(scaled_transposed_data)):\n",
    "            new_row.append(scaled_transposed_data[column_index][row_index])\n",
    "        scaled_dataset.append(new_row)\n",
    "    \n",
    "    return scaled_dataset  # Возвращаем масштабированные данные\n",
    "\n",
    "# Масштабируем признаки dataset\n",
    "scaled_dataset = scale_features(processed_dataset)\n",
    "\n",
    "# Добавляем столбец единиц для intercept (свободного члена)\n",
    "dataset_with_intercept = []\n",
    "for data_row in scaled_dataset:\n",
    "    new_row = [1]  # Добавляем единицу в начало для intercept\n",
    "    # Копируем все признаки кроме целевой переменной\n",
    "    for feature_index in range(len(data_row) - 1):\n",
    "        new_row.append(data_row[feature_index])\n",
    "    # Добавляем целевую переменную в конец\n",
    "    new_row.append(data_row[-1])\n",
    "    dataset_with_intercept.append(new_row)\n",
    "\n",
    "# Выводим информацию о размерности данных\n",
    "print(\"Добавлен intercept, размерность данных: \" + str(len(dataset_with_intercept[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351ba79-d42c-4d3f-a635-0b5d633739ad",
   "metadata": {},
   "source": [
    "#### ЗАДАНИЕ 2: РАЗДЕЛЕНИЕ ДАННЫХ НА ОБУЧАЮЩУЮ И ТЕСТОВУЮ ВЫБОРКИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48a0a76-b16a-4858-85d3-66af2beead6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: 643 примеров\n",
      "Тестовая выборка: 275 примеров\n",
      "Пропорции классов в обучающей: 0=287, 1=356\n"
     ]
    }
   ],
   "source": [
    "# Разделение данных на обучающую и тестовую выборки\n",
    "def train_test_split(input_data, test_fraction=0.3):\n",
    "    \"\"\"Разделяет данные на обучающую и тестовую выборки со стратификацией\"\"\"\n",
    "    # Разделяем данные по классам для сохранения пропорций\n",
    "    class_0_samples = []  # Создаем список для образцов класса 0 (нет болезни)\n",
    "    class_1_samples = []  # Создаем список для образцов класса 1 (есть болезнь)\n",
    "    \n",
    "    # Распределяем образцы по классам\n",
    "    for data_sample in input_data:\n",
    "        if data_sample[-1] == 0:\n",
    "            class_0_samples.append(data_sample)\n",
    "        else:\n",
    "            class_1_samples.append(data_sample)\n",
    "    \n",
    "    # Перемешиваем данные в каждом классе\n",
    "    random.shuffle(class_0_samples)\n",
    "    random.shuffle(class_1_samples)\n",
    "    \n",
    "    # Вычисляем количество образцов для теста в каждом классе\n",
    "    test_samples_class_0 = int(len(class_0_samples) * test_fraction)\n",
    "    test_samples_class_1 = int(len(class_1_samples) * test_fraction)\n",
    "    \n",
    "    # Формируем тестовую выборку\n",
    "    test_dataset = []\n",
    "    for sample_index in range(test_samples_class_0):\n",
    "        test_dataset.append(class_0_samples[sample_index])\n",
    "    for sample_index in range(test_samples_class_1):\n",
    "        test_dataset.append(class_1_samples[sample_index])\n",
    "    \n",
    "    # Формируем обучающую выборку\n",
    "    train_dataset = []\n",
    "    for sample_index in range(test_samples_class_0, len(class_0_samples)):\n",
    "        train_dataset.append(class_0_samples[sample_index])\n",
    "    for sample_index in range(test_samples_class_1, len(class_1_samples)):\n",
    "        train_dataset.append(class_1_samples[sample_index])\n",
    "    \n",
    "    # Перемешиваем итоговые выборки\n",
    "    random.shuffle(test_dataset)\n",
    "    random.shuffle(train_dataset)\n",
    "    \n",
    "    # Разделяем на признаки (X) и метки (y)\n",
    "    train_features = []  # Признаки обучающей выборки\n",
    "    train_labels = []    # Метки обучающей выборки\n",
    "    for data_sample in train_dataset:\n",
    "        feature_vector = []\n",
    "        for feature_index in range(len(data_sample) - 1):\n",
    "            feature_vector.append(data_sample[feature_index])\n",
    "        train_features.append(feature_vector)\n",
    "        train_labels.append(data_sample[-1])\n",
    "    \n",
    "    test_features = []   # Признаки тестовой выборки\n",
    "    test_labels = []     # Метки тестовой выборки\n",
    "    for data_sample in test_dataset:\n",
    "        feature_vector = []\n",
    "        for feature_index in range(len(data_sample) - 1):\n",
    "            feature_vector.append(data_sample[feature_index])\n",
    "        test_features.append(feature_vector)\n",
    "        test_labels.append(data_sample[-1])\n",
    "    \n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(dataset_with_intercept, test_fraction=0.3)\n",
    "\n",
    "# Подсчитываем количество образцов каждого класса в обучающей выборке\n",
    "class_0_count_train = 0\n",
    "class_1_count_train = 0\n",
    "for label_value in train_labels:\n",
    "    if label_value == 0:\n",
    "        class_0_count_train = class_0_count_train + 1\n",
    "    else:\n",
    "        class_1_count_train = class_1_count_train + 1\n",
    "\n",
    "# Выводим информацию о разделении данных\n",
    "print(\"Обучающая выборка: \" + str(len(train_features)) + \" примеров\")\n",
    "print(\"Тестовая выборка: \" + str(len(test_features)) + \" примеров\")\n",
    "print(\"Пропорции классов в обучающей: 0=\" + str(class_0_count_train) + \", 1=\" + str(class_1_count_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7fbbc-8da1-4444-b500-358739940f11",
   "metadata": {},
   "source": [
    "#### ЗАДАНИЕ 3: РЕАЛИЗАЦИЯ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ И ОБУЧЕНИЕ МОДЕЛИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b417166b-0e21-4c48-8937-2f76d5802dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для логистической регрессии\n",
    "def sigmoid(input_value):\n",
    "    \"\"\"Сигмоидная функция - преобразует любое число в интервал (0, 1)\"\"\"\n",
    "    # Проверяем на переполнение для очень маленьких значений\n",
    "    if input_value < -700:\n",
    "        return 0.0\n",
    "    # Проверяем на переполнение для очень больших значений\n",
    "    if input_value > 700:\n",
    "        return 1.0\n",
    "    # Вычисляем значение сигмоидной функции\n",
    "    return 1.0 / (1.0 + math.exp(-input_value))\n",
    "\n",
    "def log_likelihood_single_sample(feature_vector, true_label, model_parameters):\n",
    "    \"\"\"Логарифм правдоподобия для одного наблюдения с защитой от крайних значений\"\"\"\n",
    "    # Вычисляем линейную комбинацию признаков и параметров\n",
    "    linear_combination = dot_product(feature_vector, model_parameters)\n",
    "    # Преобразуем в вероятность с помощью сигмоидной функции\n",
    "    predicted_probability = sigmoid(linear_combination)\n",
    "    \n",
    "    # Добавляем защиту от крайних значений вероятности\n",
    "    epsilon = 1e-15  # Малое число для избежания логарифма от 0\n",
    "    if predicted_probability < epsilon:\n",
    "        predicted_probability = epsilon\n",
    "    if predicted_probability > 1 - epsilon:\n",
    "        predicted_probability = 1 - epsilon\n",
    "    \n",
    "    # Вычисляем логарифм правдоподобия в зависимости от истинной метки\n",
    "    if true_label == 1:\n",
    "        return math.log(predicted_probability)  # Логарифм P(y=1|x)\n",
    "    else:\n",
    "        return math.log(1 - predicted_probability)  # Логарифм P(y=0|x)\n",
    "\n",
    "def log_likelihood_total(feature_matrix, label_vector, model_parameters):\n",
    "    \"\"\"Суммарный логарифм правдоподобия для всей выборки\"\"\"\n",
    "    total_log_likelihood = 0.0  # Инициализируем суммарное правдоподобие\n",
    "    # Проходим по всем наблюдениям в выборке\n",
    "    for sample_index in range(len(feature_matrix)):\n",
    "        # Вычисляем правдоподобие для текущего наблюдения\n",
    "        sample_likelihood = log_likelihood_single_sample(\n",
    "            feature_matrix[sample_index], \n",
    "            label_vector[sample_index], \n",
    "            model_parameters\n",
    "        )\n",
    "        total_log_likelihood = total_log_likelihood + sample_likelihood\n",
    "    return total_log_likelihood  # Возвращаем суммарное правдоподобие\n",
    "\n",
    "def log_gradient_single_sample(feature_vector, true_label, model_parameters):\n",
    "    \"\"\"Градиент функции правдоподобия для одного наблюдения\"\"\"\n",
    "    # Вычисляем линейную комбинацию\n",
    "    linear_combination = dot_product(feature_vector, model_parameters)\n",
    "    # Вычисляем предсказанную вероятность\n",
    "    predicted_probability = sigmoid(linear_combination)\n",
    "    # Вычисляем градиент для каждого параметра\n",
    "    gradient_vector = []\n",
    "    for feature_value in feature_vector:\n",
    "        gradient_component = (true_label - predicted_probability) * feature_value\n",
    "        gradient_vector.append(gradient_component)\n",
    "    return gradient_vector  # Возвращаем вектор градиента\n",
    "\n",
    "def log_gradient_total(feature_matrix, label_vector, model_parameters):\n",
    "    \"\"\"Полный градиент функции правдоподобия для всей выборки\"\"\"\n",
    "    number_of_parameters = len(model_parameters)  # Количество параметров модели\n",
    "    total_gradient = [0.0] * number_of_parameters  # Инициализируем градиент нулями\n",
    "    # Суммируем градиенты для всех наблюдений\n",
    "    for sample_index in range(len(feature_matrix)):\n",
    "        # Вычисляем градиент для текущего наблюдения\n",
    "        sample_gradient = log_gradient_single_sample(\n",
    "            feature_matrix[sample_index], \n",
    "            label_vector[sample_index], \n",
    "            model_parameters\n",
    "        )\n",
    "        # Добавляем к общему градиенту\n",
    "        for param_index in range(number_of_parameters):\n",
    "            total_gradient[param_index] = total_gradient[param_index] + sample_gradient[param_index]\n",
    "    return total_gradient  # Возвращаем полный градиент\n",
    "\n",
    "def negate_function(original_function):\n",
    "    \"\"\"Возвращает функцию, которая возвращает отрицательное значение исходной\"\"\"\n",
    "    def negated_function(*args, **kwargs):\n",
    "        return -original_function(*args, **kwargs)  # Возвращаем отрицательное значение\n",
    "    return negated_function  # Возвращаем новую функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8344646-0616-4c32-ab30-62b5a2cdee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начальные параметры: ['-0.0144', '-0.0127', '-0.0279', '0.0281', '0.0406', '0.0089', '-0.0242', '-0.0079', '-0.0148', '0.0494', '-0.0421', '0.0465']\n",
      "Обучение логистической регрессии с улучшенным алгоритмом...\n",
      "Начальная функция потерь: 435.4645\n",
      "Итерация 0, Функция потерь: 435.0939\n",
      "Итерация 100, Функция потерь: 400.3084\n",
      "Итерация 200, Функция потерь: 369.9326\n",
      "Итерация 300, Функция потерь: 343.7524\n",
      "Итерация 400, Функция потерь: 321.4706\n",
      "Итерация 500, Функция потерь: 302.7375\n",
      "Итерация 600, Функция потерь: 287.1790\n",
      "Итерация 700, Функция потерь: 274.4205\n",
      "Итерация 800, Функция потерь: 264.1024\n",
      "Итерация 900, Функция потерь: 255.8905\n",
      "Итерация 1000, Функция потерь: 249.4812\n",
      "Итерация 1100, Функция потерь: 244.6030\n",
      "Итерация 1200, Функция потерь: 241.0165\n",
      "Итерация 1300, Функция потерь: 238.5145\n",
      "Итерация 1400, Функция потерь: 236.9201\n",
      "Итерация 1500, Функция потерь: 236.0856\n",
      "Итерация 1600, Функция потерь: 235.8746\n",
      "Итерация 1700, Функция потерь: 235.8739\n",
      "Итерация 1800, Функция потерь: 235.8739\n",
      "Итерация 1900, Функция потерь: 235.8739\n",
      "Лучшая функция потерь: 235.8739\n",
      "Обученные параметры: ['0.3654', '0.0158', '0.4801', '0.6104', '0.1013', '-0.3226', '0.3952', '-0.0918', '-0.4680', '0.5829', '0.4256', '0.8034']\n"
     ]
    }
   ],
   "source": [
    "# Обучение логистической регрессии\n",
    "# Создаем оберточные функции для совместимости с gradient_descent\n",
    "def loss_function_total(feature_matrix, label_vector, model_parameters):\n",
    "    \"\"\"Функция потерь (отрицательное правдоподобие) для всей выборки\"\"\"\n",
    "    # Вычисляем отрицательное правдоподобие как функцию потерь\n",
    "    negative_log_likelihood = -log_likelihood_total(feature_matrix, label_vector, model_parameters)\n",
    "    return negative_log_likelihood  # Возвращаем значение функции потерь\n",
    "\n",
    "def loss_gradient_total(feature_matrix, label_vector, model_parameters):\n",
    "    \"\"\"Градиент функции потерь для всей выборки\"\"\"\n",
    "    # Вычисляем градиент правдоподобия\n",
    "    likelihood_gradient = log_gradient_total(feature_matrix, label_vector, model_parameters)\n",
    "    # Преобразуем в градиент потерь (отрицательный градиент правдоподобия)\n",
    "    loss_gradient_vector = []\n",
    "    for gradient_component in likelihood_gradient:\n",
    "        loss_gradient_vector.append(-gradient_component)\n",
    "    return loss_gradient_vector  # Возвращаем градиент функции потерь\n",
    "\n",
    "# Улучшенная функция градиентного спуска\n",
    "def improved_gradient_descent(function_to_minimize, initial_point, learning_rate=0.001, max_iterations=1000):\n",
    "    \"\"\"Улучшенная реализация градиентного спуска с защитой от расходимости\"\"\"\n",
    "    current_point = []  # Создаем список для текущей точки\n",
    "    # Копируем начальную точку\n",
    "    for value in initial_point:\n",
    "        current_point.append(value)\n",
    "    \n",
    "    best_point = []  # Создаем список для лучшей точки\n",
    "    for value in current_point:\n",
    "        best_point.append(value)\n",
    "    best_loss = function_to_minimize(current_point)  # Вычисляем начальную функцию потерь\n",
    "    \n",
    "    print(\"Начальная функция потерь: \" + \"{:.4f}\".format(best_loss))\n",
    "    \n",
    "    # Выполняем итерации градиентного спуска\n",
    "    for iteration in range(max_iterations):\n",
    "        # Вычисляем градиент в текущей точке\n",
    "        current_gradient = gradient(function_to_minimize, current_point)\n",
    "        \n",
    "        # Ограничиваем норму градиента для стабильности\n",
    "        gradient_norm = 0.0\n",
    "        for gradient_component in current_gradient:\n",
    "            gradient_norm = gradient_norm + gradient_component * gradient_component\n",
    "        gradient_norm = math.sqrt(gradient_norm)\n",
    "        \n",
    "        # Если градиент слишком большой, нормализуем его\n",
    "        if gradient_norm > 1.0:\n",
    "            normalization_factor = 1.0 / gradient_norm\n",
    "            for i in range(len(current_gradient)):\n",
    "                current_gradient[i] = current_gradient[i] * normalization_factor\n",
    "        \n",
    "        # Делаем шаг в направлении, противоположном градиенту\n",
    "        current_point = vector_add(current_point, scalar_multiply(-learning_rate, current_gradient))\n",
    "        \n",
    "        # Вычисляем текущую функцию потерь\n",
    "        current_loss = function_to_minimize(current_point)\n",
    "        \n",
    "        # Сохраняем лучшую точку\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            for i in range(len(current_point)):\n",
    "                best_point[i] = current_point[i]\n",
    "        \n",
    "        # Выводим прогресс каждые 100 итераций\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"Итерация \" + str(iteration) + \", Функция потерь: \" + \"{:.4f}\".format(current_loss))\n",
    "            \n",
    "        # Ранняя остановка если потери начали расти\n",
    "        if current_loss > best_loss * 2.0:  # Если потери выросли в 2 раза\n",
    "            print(\"Ранняя остановка на итерации \" + str(iteration) + \" - функция потерь растет\")\n",
    "            break\n",
    "    \n",
    "    print(\"Лучшая функция потерь: \" + \"{:.4f}\".format(best_loss))\n",
    "    return best_point  # Возвращаем лучшую найденную точку\n",
    "\n",
    "# Инициализируем начальные значения параметров модели с меньшими значениями\n",
    "number_of_features = len(train_features[0])  # Количество признаков (включая intercept)\n",
    "initial_parameters = []  # Создаем список для начальных параметров\n",
    "# Инициализируем параметры маленькими случайными значениями\n",
    "for parameter_index in range(number_of_features):\n",
    "    random_value = (random.random() - 0.5) * 0.1  # Случайные значения от -0.05 до 0.05\n",
    "    initial_parameters.append(random_value)\n",
    "\n",
    "# Форматируем начальные параметры для красивого вывода\n",
    "formatted_initial_parameters = []\n",
    "for parameter_value in initial_parameters:\n",
    "    formatted_value = \"{:.4f}\".format(parameter_value)  # Форматируем до 4 знаков после запятой\n",
    "    formatted_initial_parameters.append(formatted_value)\n",
    "print(\"Начальные параметры: \" + str(formatted_initial_parameters))\n",
    "\n",
    "# Обучаем модель логистической регрессии\n",
    "print(\"Обучение логистической регрессии с улучшенным алгоритмом...\")\n",
    "# Создаем функцию потерь с фиксированными обучающими данными\n",
    "def loss_function_for_optimization(model_parameters):\n",
    "    return loss_function_total(train_features, train_labels, model_parameters)\n",
    "\n",
    "# Выполняем градиентный спуск для минимизации функции потерь с улучшенной функцией\n",
    "optimized_parameters = improved_gradient_descent(\n",
    "    loss_function_for_optimization,  # Функция для минимизации\n",
    "    initial_parameters,              # Начальные значения параметров\n",
    "    learning_rate=0.001,            # Малая скорость обучения для стабильности\n",
    "    max_iterations=2000             # Максимальное количество итераций\n",
    ")\n",
    "\n",
    "# Форматируем обученные параметры для красивого вывода\n",
    "formatted_optimized_parameters = []\n",
    "for parameter_value in optimized_parameters:\n",
    "    formatted_value = \"{:.4f}\".format(parameter_value)  # Форматируем до 4 знаков после запятой\n",
    "    formatted_optimized_parameters.append(formatted_value)\n",
    "print(\"Обученные параметры: \" + str(formatted_optimized_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1379e038-cbb3-4a5e-bbfe-2edcae283eb5",
   "metadata": {},
   "source": [
    "#### ЗАДАНИЕ 4-5: ПРЕДСКАЗАНИЯ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ И ВЫДЕЛЕНИЕ НЕСТАБИЛЬНЫХ СЛУЧАЕВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf02947-6c15-4c92-bdcb-ad1036ec7d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нестабильных предсказаний: 50/275 (18.18%)\n",
      "\n",
      "Анализ распределения вероятностей:\n",
      "Минимальная вероятность: 0.0107\n",
      "Максимальная вероятность: 0.9968\n",
      "Средняя вероятность: 0.5472\n"
     ]
    }
   ],
   "source": [
    "# Предсказания с помощью логистической регрессии\n",
    "def predict_logistic_regression(feature_vector, model_parameters, classification_threshold=0.5):\n",
    "    \"\"\"Предсказание метки с помощью логистической регрессии\"\"\"\n",
    "    # Вычисляем вероятность принадлежности к классу 1\n",
    "    predicted_probability = sigmoid(dot_product(feature_vector, model_parameters))\n",
    "    # Применяем порог для классификации\n",
    "    if predicted_probability >= classification_threshold:\n",
    "        return 1  # Класс 1 (есть заболевание сердца)\n",
    "    else:\n",
    "        return 0  # Класс 0 (нет заболевания сердца)\n",
    "\n",
    "def predict_logistic_regression_with_uncertainty(feature_vector, model_parameters, low_threshold=0.3, high_threshold=0.7):\n",
    "    \"\"\"Предсказание с выделением нестабильных случаев с более широкой серой зоной\"\"\"\n",
    "    # Вычисляем вероятность принадлежности к классу 1\n",
    "    predicted_probability = sigmoid(dot_product(feature_vector, model_parameters))\n",
    "    # Определяем класс на основе вероятности и порогов\n",
    "    if predicted_probability >= low_threshold and predicted_probability <= high_threshold:\n",
    "        return 2  # Нестабильный случай (требует дополнительной проверки)\n",
    "    elif predicted_probability > high_threshold:\n",
    "        return 1  # Класс 1 (высокая уверенность в наличии заболевания)\n",
    "    else:\n",
    "        return 0  # Класс 0 (высокая уверенность в отсутствии заболевания)\n",
    "\n",
    "# Делаем предсказания на тестовой выборке с помощью логистической регрессии\n",
    "logistic_regression_predictions_binary = []  # Список для бинарных предсказаний\n",
    "for test_feature_vector in test_features:\n",
    "    # Предсказываем метку для каждого тестового примера\n",
    "    binary_prediction = predict_logistic_regression(test_feature_vector, optimized_parameters)\n",
    "    logistic_regression_predictions_binary.append(binary_prediction)\n",
    "\n",
    "logistic_regression_predictions_ternary = []  # Список для тернарных предсказаний\n",
    "for test_feature_vector in test_features:\n",
    "    # Предсказываем метку с учетом нестабильных случаев\n",
    "    ternary_prediction = predict_logistic_regression_with_uncertainty(test_feature_vector, optimized_parameters)\n",
    "    logistic_regression_predictions_ternary.append(ternary_prediction)\n",
    "\n",
    "# Анализируем нестабильные предсказания\n",
    "unstable_predictions_count = 0  # Счетчик нестабильных предсказаний\n",
    "for prediction_value in logistic_regression_predictions_ternary:\n",
    "    if prediction_value == 2:  # Если предсказание нестабильное\n",
    "        unstable_predictions_count = unstable_predictions_count + 1\n",
    "\n",
    "total_predictions_count = len(logistic_regression_predictions_ternary)  # Общее количество предсказаний\n",
    "# Вычисляем процент нестабильных предсказаний\n",
    "unstable_percentage = (unstable_predictions_count / total_predictions_count) * 100\n",
    "\n",
    "# Выводим статистику по нестабильным предсказаниям\n",
    "print(\"Нестабильных предсказаний: \" + str(unstable_predictions_count) + \"/\" + str(total_predictions_count) + \n",
    "      \" (\" + \"{:.2f}\".format(unstable_percentage) + \"%)\")\n",
    "\n",
    "# Анализируем распределение вероятностей для отладки\n",
    "print(\"\\nАнализ распределения вероятностей:\")\n",
    "probability_values = []\n",
    "for test_feature_vector in test_features:\n",
    "    probability = sigmoid(dot_product(test_feature_vector, optimized_parameters))\n",
    "    probability_values.append(probability)\n",
    "\n",
    "min_prob = min(probability_values)\n",
    "max_prob = max(probability_values)\n",
    "mean_prob = sum(probability_values) / len(probability_values)\n",
    "print(\"Минимальная вероятность: \" + \"{:.4f}\".format(min_prob))\n",
    "print(\"Максимальная вероятность: \" + \"{:.4f}\".format(max_prob))\n",
    "print(\"Средняя вероятность: \" + \"{:.4f}\".format(mean_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b87cdb-248b-429f-85e5-a8c996df1822",
   "metadata": {},
   "source": [
    "#### ЗАДАНИЕ 6: РЕАЛИЗАЦИЯ МЕТОДА БЛИЖАЙШИХ СОСЕДЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6154bae1-59f9-4e53-aedf-3e71d55abe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Применение метода ближайших соседей...\n",
      "Предсказания KNN завершены\n"
     ]
    }
   ],
   "source": [
    "# Реализация метода ближайших соседей\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    \"\"\"Вычисляет евклидово расстояние между двумя векторами одинаковой длины\"\"\"\n",
    "    sum_squared_differences = 0.0  # Инициализируем сумму квадратов разностей\n",
    "    # Проходим по всем элементам векторов\n",
    "    for element_index in range(len(vector1)):\n",
    "        # Вычисляем разность между соответствующими элементами\n",
    "        difference = vector1[element_index] - vector2[element_index]\n",
    "        # Добавляем квадрат разности к сумме\n",
    "        sum_squared_differences = sum_squared_differences + difference * difference\n",
    "    # Извлекаем квадратный корень из суммы квадратов разностей\n",
    "    distance = math.sqrt(sum_squared_differences)\n",
    "    return distance  # Возвращаем евклидово расстояние\n",
    "\n",
    "def find_k_nearest_neighbors(train_features_matrix, train_labels_vector, test_instance, number_of_neighbors):\n",
    "    \"\"\"Находит k ближайших соседей для тестового примера\"\"\"\n",
    "    distances_list = []  # Создаем список для хранения расстояний\n",
    "    # Вычисляем расстояния до всех обучающих примеров\n",
    "    for train_index in range(len(train_features_matrix)):\n",
    "        train_feature_vector = train_features_matrix[train_index]  # Признаки текущего обучающего примера\n",
    "        train_label_value = train_labels_vector[train_index]        # Метка текущего обучающего примера\n",
    "        # Вычисляем расстояние от тестового примера до текущего обучающего\n",
    "        current_distance = euclidean_distance(test_instance, train_feature_vector)\n",
    "        # Сохраняем пример, его метку и расстояние\n",
    "        distances_list.append((train_feature_vector, train_label_value, current_distance))\n",
    "    \n",
    "    # Сортируем список по расстоянию (от ближайшего к дальнему)\n",
    "    def get_distance(element):\n",
    "        return element[2]  # Возвращаем расстояние для сортировки\n",
    "    distances_list.sort(key=get_distance)\n",
    "    \n",
    "    # Выбираем k ближайших соседей\n",
    "    nearest_neighbors = []\n",
    "    for neighbor_index in range(number_of_neighbors):\n",
    "        nearest_neighbors.append(distances_list[neighbor_index])\n",
    "    return nearest_neighbors  # Возвращаем список ближайших соседей\n",
    "\n",
    "def predict_k_nearest_neighbors(train_features_matrix, train_labels_vector, test_instance, number_of_neighbors=5):\n",
    "    \"\"\"Предсказание метки с помощью метода k ближайших соседей\"\"\"\n",
    "    # Находим k ближайших соседей\n",
    "    neighbors_list = find_k_nearest_neighbors(train_features_matrix, train_labels_vector, test_instance, number_of_neighbors)\n",
    "    \n",
    "    votes_dictionary = {}  # Создаем словарь для подсчета голосов\n",
    "    # Подсчитываем голоса соседей\n",
    "    for neighbor_tuple in neighbors_list:\n",
    "        neighbor_label = neighbor_tuple[1]  # Получаем метку соседа\n",
    "        # Увеличиваем счетчик голосов для этой метки\n",
    "        if neighbor_label in votes_dictionary:\n",
    "            votes_dictionary[neighbor_label] = votes_dictionary[neighbor_label] + 1\n",
    "        else:\n",
    "            votes_dictionary[neighbor_label] = 1\n",
    "    \n",
    "    # Находим метку с наибольшим числом голосов\n",
    "    best_label = None\n",
    "    max_votes = -1\n",
    "    for label_value, vote_count in votes_dictionary.items():\n",
    "        if vote_count > max_votes:\n",
    "            max_votes = vote_count\n",
    "            best_label = label_value\n",
    "    \n",
    "    return best_label  # Возвращаем метку с наибольшим числом голосов\n",
    "\n",
    "# Подготавливаем данные для KNN (убираем intercept, так как он не нужен для расстояний)\n",
    "train_features_for_knn = []  # Признаки обучающей выборки для KNN\n",
    "for train_feature_vector in train_features:\n",
    "    feature_vector_without_intercept = []\n",
    "    # Пропускаем первый элемент (intercept)\n",
    "    for feature_index in range(1, len(train_feature_vector)):\n",
    "        feature_vector_without_intercept.append(train_feature_vector[feature_index])\n",
    "    train_features_for_knn.append(feature_vector_without_intercept)\n",
    "\n",
    "test_features_for_knn = []  # Признаки тестовой выборки для KNN\n",
    "for test_feature_vector in test_features:\n",
    "    feature_vector_without_intercept = []\n",
    "    # Пропускаем первый элемент (intercept)\n",
    "    for feature_index in range(1, len(test_feature_vector)):\n",
    "        feature_vector_without_intercept.append(test_feature_vector[feature_index])\n",
    "    test_features_for_knn.append(feature_vector_without_intercept)\n",
    "\n",
    "# Делаем предсказания методом ближайших соседей\n",
    "print(\"Применение метода ближайших соседей...\")\n",
    "knn_predictions = []  # Список для предсказаний KNN\n",
    "for test_feature_vector in test_features_for_knn:\n",
    "    # Предсказываем метку для каждого тестового примера\n",
    "    knn_prediction = predict_k_nearest_neighbors(train_features_for_knn, train_labels, test_feature_vector, number_of_neighbors=5)\n",
    "    knn_predictions.append(knn_prediction)\n",
    "\n",
    "print(\"Предсказания KNN завершены\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955e697-2390-4503-a391-b9238302b962",
   "metadata": {},
   "source": [
    "#### ЗАДАНИЕ 7: ВЫЧИСЛЕНИЕ МЕТРИК КАЧЕСТВА КЛАССИФИКАЦИИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3197869f-88d0-4838-bbc2-560bb9c328e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ (ТЕРНАРНАЯ) ---\n",
      "Стабильные предсказания: 225/275\n",
      "Правильные стабильные предсказания: 204/225\n",
      "\n",
      "--- МЕТРИКИ КАЧЕСТВА ---\n",
      "Логистическая регрессия (бинарная):\n",
      "  Precision: 0.8800\n",
      "  Recall:    0.8684\n",
      "  F1-score:  0.8742\n",
      "  Accuracy:  0.8618\n",
      "  TP: 132, FP: 18, TN: 105, FN: 20\n",
      "\n",
      "Логистическая регрессия (тернарная):\n",
      "  Accuracy (без нестабильных): 0.9067\n",
      "\n",
      "Метод ближайших соседей (k=5):\n",
      "  Precision: 0.8947\n",
      "  Recall:    0.8947\n",
      "  F1-score:  0.8947\n",
      "  Accuracy:  0.8836\n",
      "  TP: 136, FP: 16, TN: 107, FN: 16\n"
     ]
    }
   ],
   "source": [
    "# Вычисление метрик качества\n",
    "def calculate_binary_classification_metrics(true_labels_vector, predicted_labels_vector):\n",
    "    \"\"\"Вычисление метрик для бинарной классификации\"\"\"\n",
    "    # Инициализируем счетчики матрицы ошибок\n",
    "    true_positives = 0  # Истинные положительные (правильно предсказанные больные)\n",
    "    false_positives = 0 # Ложные положительные (здоровые, предсказанные как больные)\n",
    "    true_negatives = 0  # Истинные отрицательные (правильно предсказанные здоровые)\n",
    "    false_negatives = 0 # Ложные отрицательные (больные, предсказанные как здоровые)\n",
    "    \n",
    "    # Подсчитываем элементы матрицы ошибок\n",
    "    for sample_index in range(len(true_labels_vector)):\n",
    "        true_label = true_labels_vector[sample_index]\n",
    "        predicted_label = predicted_labels_vector[sample_index]\n",
    "        if true_label == 1 and predicted_label == 1:\n",
    "            true_positives = true_positives + 1\n",
    "        elif true_label == 1 and predicted_label == 0:\n",
    "            false_negatives = false_negatives + 1\n",
    "        elif true_label == 0 and predicted_label == 1:\n",
    "            false_positives = false_positives + 1\n",
    "        elif true_label == 0 and predicted_label == 0:\n",
    "            true_negatives = true_negatives + 1\n",
    "    \n",
    "    # Вычисляем метрики качества (с проверкой деления на ноль)\n",
    "    if (true_positives + false_positives) > 0:\n",
    "        precision_metric = true_positives / (true_positives + false_positives)\n",
    "    else:\n",
    "        precision_metric = 0.0\n",
    "    \n",
    "    if (true_positives + false_negatives) > 0:\n",
    "        recall_metric = true_positives / (true_positives + false_negatives)\n",
    "    else:\n",
    "        recall_metric = 0.0\n",
    "    \n",
    "    if (precision_metric + recall_metric) > 0:\n",
    "        f1_score_metric = 2 * precision_metric * recall_metric / (precision_metric + recall_metric)\n",
    "    else:\n",
    "        f1_score_metric = 0.0\n",
    "    \n",
    "    if len(true_labels_vector) > 0:\n",
    "        accuracy_metric = (true_positives + true_negatives) / len(true_labels_vector)\n",
    "    else:\n",
    "        accuracy_metric = 0.0\n",
    "    \n",
    "    # Возвращаем словарь с вычисленными метриками\n",
    "    metrics_dictionary = {\n",
    "        'precision': precision_metric,\n",
    "        'recall': recall_metric,\n",
    "        'f1': f1_score_metric,\n",
    "        'accuracy': accuracy_metric,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'true_negatives': true_negatives,\n",
    "        'false_negatives': false_negatives\n",
    "    }\n",
    "    return metrics_dictionary\n",
    "\n",
    "def calculate_ternary_classification_accuracy(true_labels_vector, ternary_predictions_vector):\n",
    "    \"\"\"Точность для тернарной классификации (игнорируя нестабильные случаи)\"\"\"\n",
    "    correct_predictions_count = 0  # Счетчик правильных предсказаний\n",
    "    total_stable_predictions_count = 0  # Счетчик стабильных предсказаний (исключая нестабильные)\n",
    "    \n",
    "    # Подсчитываем правильные предсказания только для стабильных случаев\n",
    "    for sample_index in range(len(true_labels_vector)):\n",
    "        ternary_prediction = ternary_predictions_vector[sample_index]\n",
    "        # Игнорируем нестабильные предсказания (класс 2)\n",
    "        if ternary_prediction != 2:\n",
    "            total_stable_predictions_count = total_stable_predictions_count + 1\n",
    "            true_label = true_labels_vector[sample_index]\n",
    "            if true_label == ternary_prediction:\n",
    "                correct_predictions_count = correct_predictions_count + 1\n",
    "                \n",
    "    # Вычисляем точность для стабильных предсказаний\n",
    "    if total_stable_predictions_count > 0:\n",
    "        ternary_accuracy = correct_predictions_count / total_stable_predictions_count\n",
    "        # Выводим дополнительную информацию\n",
    "        print(\"Стабильные предсказания: \" + str(total_stable_predictions_count) + \"/\" + str(len(true_labels_vector)))\n",
    "        print(\"Правильные стабильные предсказания: \" + str(correct_predictions_count) + \"/\" + str(total_stable_predictions_count))\n",
    "    else:\n",
    "        ternary_accuracy = 0.0\n",
    "        print(\"Нет стабильных предсказаний для вычисления точности\")\n",
    "        \n",
    "    return ternary_accuracy  # Возвращаем точность\n",
    "\n",
    "# Вычисляем метрики для логистической регрессии (бинарная классификация)\n",
    "logistic_regression_binary_metrics = calculate_binary_classification_metrics(test_labels, logistic_regression_predictions_binary)\n",
    "\n",
    "# Вычисляем точность для тернарной классификации логистической регрессии\n",
    "print(\"\\n--- ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ (ТЕРНАРНАЯ) ---\")\n",
    "logistic_regression_ternary_accuracy = calculate_ternary_classification_accuracy(test_labels, logistic_regression_predictions_ternary)\n",
    "\n",
    "# Вычисляем метрики для метода ближайших соседей\n",
    "knn_metrics = calculate_binary_classification_metrics(test_labels, knn_predictions)\n",
    "\n",
    "# Выводим результаты вычисления метрик\n",
    "print(\"\\n--- МЕТРИКИ КАЧЕСТВА ---\")\n",
    "print(\"Логистическая регрессия (бинарная):\")\n",
    "print(\"  Precision: \" + \"{:.4f}\".format(logistic_regression_binary_metrics['precision']))\n",
    "print(\"  Recall:    \" + \"{:.4f}\".format(logistic_regression_binary_metrics['recall']))\n",
    "print(\"  F1-score:  \" + \"{:.4f}\".format(logistic_regression_binary_metrics['f1']))\n",
    "print(\"  Accuracy:  \" + \"{:.4f}\".format(logistic_regression_binary_metrics['accuracy']))\n",
    "print(\"  TP: \" + str(logistic_regression_binary_metrics['true_positives']) + \n",
    "      \", FP: \" + str(logistic_regression_binary_metrics['false_positives']) +\n",
    "      \", TN: \" + str(logistic_regression_binary_metrics['true_negatives']) +\n",
    "      \", FN: \" + str(logistic_regression_binary_metrics['false_negatives']))\n",
    "\n",
    "print(\"\\nЛогистическая регрессия (тернарная):\")\n",
    "print(\"  Accuracy (без нестабильных): \" + \"{:.4f}\".format(logistic_regression_ternary_accuracy))\n",
    "\n",
    "print(\"\\nМетод ближайших соседей (k=5):\")\n",
    "print(\"  Precision: \" + \"{:.4f}\".format(knn_metrics['precision']))\n",
    "print(\"  Recall:    \" + \"{:.4f}\".format(knn_metrics['recall']))\n",
    "print(\"  F1-score:  \" + \"{:.4f}\".format(knn_metrics['f1']))\n",
    "print(\"  Accuracy:  \" + \"{:.4f}\".format(knn_metrics['accuracy']))\n",
    "print(\"  TP: \" + str(knn_metrics['true_positives']) + \n",
    "      \", FP: \" + str(knn_metrics['false_positives']) +\n",
    "      \", TN: \" + str(knn_metrics['true_negatives']) +\n",
    "      \", FN: \" + str(knn_metrics['false_negatives']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f10546-3de7-4561-85cb-bd26b0a0f41e",
   "metadata": {},
   "source": [
    "#### ЗАДАНИЕ 8: ИНТЕРПРЕТАЦИЯ РЕЗУЛЬТАТОВ И ВЫВОДЫ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e82b1d4-005a-498f-84fe-f717e9ae50ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ИНТЕРПРЕТАЦИЯ РЕЗУЛЬТАТОВ ---\n",
      "1. Сравнение методов классификации:\n",
      "   - Логистическая регрессия показывает Accuracy = 0.8618\n",
      "   - Метод ближайших соседей показывает Accuracy = 0.8836\n",
      "\n",
      "2. Анализ нестабильных предсказаний:\n",
      "   - 18.18% примеров классифицируются как нестабильные\n",
      "   - Эти случаи находятся в 'серой зоне' и требуют дополнительного внимания врача\n",
      "   - Нестабильные случаи могут указывать на пограничные состояния пациентов\n",
      "\n",
      "3. Качество классификации по метрикам:\n",
      "   - Precision: доля правильно предсказанных больных среди всех предсказанных больных\n",
      "   - Recall: доля правильно найденных больных среди всех реально больных\n",
      "   - F1-score: гармоническое среднее между Precision и Recall\n",
      "   - Accuracy: общая доля правильных предсказаний\n",
      "\n",
      "4. Выводы о методах классификации:\n",
      "   - Оба метода демонстрируют сопоставимое качество классификации\n",
      "   - Логистическая регрессия более интерпретируема - можно анализировать веса признаков\n",
      "   - Метод ближайших соседей не требует обучения, но медленнее на этапе предсказания\n",
      "   - Введение третьего класса (нестабильные) повышает надежность системы\n",
      "   - Для критических медицинских решений важна не только точность, но и интерпретируемость\n"
     ]
    }
   ],
   "source": [
    "# Интерпретация результатов\n",
    "print(\"\\n--- ИНТЕРПРЕТАЦИЯ РЕЗУЛЬТАТОВ ---\")\n",
    "print(\"1. Сравнение методов классификации:\")\n",
    "print(\"   - Логистическая регрессия показывает Accuracy = \" + \n",
    "      \"{:.4f}\".format(logistic_regression_binary_metrics['accuracy']))\n",
    "print(\"   - Метод ближайших соседей показывает Accuracy = \" + \n",
    "      \"{:.4f}\".format(knn_metrics['accuracy']))\n",
    "\n",
    "print(\"\\n2. Анализ нестабильных предсказаний:\")\n",
    "print(\"   - \" + \"{:.2f}\".format(unstable_percentage) + \n",
    "      \"% примеров классифицируются как нестабильные\")\n",
    "print(\"   - Эти случаи находятся в 'серой зоне' и требуют дополнительного внимания врача\")\n",
    "print(\"   - Нестабильные случаи могут указывать на пограничные состояния пациентов\")\n",
    "\n",
    "print(\"\\n3. Качество классификации по метрикам:\")\n",
    "print(\"   - Precision: доля правильно предсказанных больных среди всех предсказанных больных\")\n",
    "print(\"   - Recall: доля правильно найденных больных среди всех реально больных\") \n",
    "print(\"   - F1-score: гармоническое среднее между Precision и Recall\")\n",
    "print(\"   - Accuracy: общая доля правильных предсказаний\")\n",
    "\n",
    "print(\"\\n4. Выводы о методах классификации:\")\n",
    "print(\"   - Оба метода демонстрируют сопоставимое качество классификации\")\n",
    "print(\"   - Логистическая регрессия более интерпретируема - можно анализировать веса признаков\")\n",
    "print(\"   - Метод ближайших соседей не требует обучения, но медленнее на этапе предсказания\")\n",
    "print(\"   - Введение третьего класса (нестабильные) повышает надежность системы\")\n",
    "print(\"   - Для критических медицинских решений важна не только точность, но и интерпретируемость\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
